{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part1 : Build a CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import the keras libraries and packages\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Sequential : Initiate the NN, two ways: as a sequence of layers or as a graph. CNN is a sequence of layers, so we use Sequential to initialize our NN.\n",
    "- Convolution2D : Images are 2D, videos are 3D with time. Here we deal with images. So it is Convolution2D.\n",
    "- MaxPooling2D \n",
    "- Flatten : convert matric to a large vecter\n",
    "- Dense : Fully connect layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialising the CNN\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step1 : Convolution layer\n",
    "model.add(Convolution2D(32,(3,3), input_shape=(64,64,3), activation='relu' ))\n",
    "# 64 - numbers of filters (feature detectors, the numbers of feature maps)\n",
    "# (3,3) - filters' dimension, or kernel size ,(5,5),(7,7)\n",
    "# input_shape - Image size and numbers of channel\n",
    "# activation - Relu nonlinear function, no negative\n",
    "# others parameters just keep default: strides=(1, 1), padding='valid' \n",
    "# output : (62,62,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step2 : Pooling layer : in oder to reduce the numbers of input nodes before flatten this to a large vector\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "# pool_size=(2, 2), maximium value of four (2X2 square) \n",
    "# strides=None, \n",
    "# padding='valid' \n",
    "# the model reduced by 2\n",
    "# the output size = half of original size + 1 : (7,7)->(4,4) (5,5)->(3,3)   \n",
    "# output : (31,31,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step3: Flatten\n",
    "model.add(Flatten())\n",
    "# convert all of the feature maps into a large feature vector, one dimensional vector. \n",
    "# output : 31*31*64 = 61504"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step4 : Fully connection layer as hidden layer\n",
    "model.add(Dense(activation=\"relu\", units=128))\n",
    "# units = 128 : the last step generated 61504 input nodes, the final output nodes just need 1, cat or dog. \n",
    "# How much nodes in hidden layer there is no rules. \n",
    "# As general, it is between input nodes(61504) and output nodes( 1, cat or dog ). \n",
    "# But to common practice, it is better to choose a power of 2, and around 100. 128 hidden nodes is better for this case.\n",
    "# activation='relu'\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(units = 1, activation='sigmoid'))\n",
    "# units = 1 : cat or dog\n",
    "# activation='sigmoid' : it is because only have a binary outcome , if more than two categories, we need softmax \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compile the CNN\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# optimizer='adam', (sgd)\n",
    "# loss='binary_crossentropy', if more than three outcomes, use category_crossentropy\n",
    "# metrics=['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 62, 62, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 31, 31, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 30752)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               3936384   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 3,937,409\n",
      "Trainable params: 3,937,409\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part2 : Fit the CNN to the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step1: preprocessing the images\n",
    "# Before fit the  model, proceed to image augmentation (consists of pre-processing images) to prevent overfitting\n",
    "# data augmentation: random transformation on a random selection of images, like rotation, flipping, shifting, shearing...\n",
    "# data augmentation functions: 1-enrich datasets without adding more images,\n",
    "#                              2-reduce overfitting with a small amount of images\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "# rescale = 1./255: pixel value between 0-1\n",
    "# shear_range = 0.2: Through a shear mapping, a rectangle becomes a parallelogram.\n",
    "# zoom_range = 0.2:  Range for random zoom. \n",
    "# horizontal_flip = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "training_set = train_datagen.flow_from_directory('dataset/training_set',\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'binary')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory('dataset/test_set',\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'binary')\n",
    "\n",
    "# target_size = (64, 64), the images size are different, so here they are made into 64 * 64. Every images are same size.\n",
    "# (64,64) is set according to the input_shape in the first layer( step 1, part 1, convolutional layer).\n",
    "# batch_size = 32, the size of the batches in which some random samples of images will be included\n",
    "# batch_size contains the number of iamges will go through the CNN after weights updated. Every 32 images update weights once.\n",
    "# class_mode = 'binary', two classes here, so binary.  \"categorical\", \"binary\", \"sparse\" or None. Default: \"categorical\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "8000/8000 [==============================] - 3201s - loss: 0.4087 - acc: 0.8064 - val_loss: 0.5636 - val_acc: 0.7887\n",
      "Epoch 2/5\n",
      "8000/8000 [==============================] - 3011s - loss: 0.1874 - acc: 0.9247 - val_loss: 0.8110 - val_acc: 0.7713\n",
      "Epoch 3/5\n",
      "8000/8000 [==============================] - 2954s - loss: 0.1109 - acc: 0.9586 - val_loss: 0.9859 - val_acc: 0.7727\n",
      "Epoch 4/5\n",
      "8000/8000 [==============================] - 3020s - loss: 0.0780 - acc: 0.9719 - val_loss: 1.0221 - val_acc: 0.7841\n",
      "Epoch 5/5\n",
      "8000/8000 [==============================] - 2875s - loss: 0.0598 - acc: 0.9790 - val_loss: 1.2114 - val_acc: 0.7801\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11e149710>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step2: fit and test the performance\n",
    "model.fit_generator(training_set,\n",
    "                    steps_per_epoch = 8000,\n",
    "                    epochs = 5,\n",
    "                    validation_data = test_set,\n",
    "                    validation_steps = 2000)\n",
    "# generator, \n",
    "# steps_per_epoch, total numbers of images in training set \n",
    "# epochs=25, total number of iterations on the data.\n",
    "# verbose=1, 0, 1, or 2.\n",
    "# validation_data=test_set, a generator for the validation data, here is test_set\n",
    "# validation_steps=None,  Total number of steps (batches of samples) to yield from  generator before stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('./CNN_catordog.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
