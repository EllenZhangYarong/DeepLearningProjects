{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "import tensorflow\n",
    "from scipy.sparse import coo_matrix\n",
    "from sklearn.utils import shuffle\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.1\n"
     ]
    }
   ],
   "source": [
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6.0\n"
     ]
    }
   ],
   "source": [
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 . Create training set and test set\n",
    "### Training set including three part of data\n",
    "1. 60000 handwritten numbers \n",
    "2. 6000 random noise \n",
    "3. 6000 handwritten alphabet noise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_data(path, kind=''):\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path,\n",
    "                               '%s-labels-idx1-ubyte'\n",
    "                               % kind)\n",
    "    images_path = os.path.join(path,\n",
    "                               '%s-images-idx3-ubyte'\n",
    "                               % kind)\n",
    "    with open(labels_path, 'rb') as lbpath:\n",
    "        magic, n = struct.unpack('>II',\n",
    "                                 lbpath.read(8))\n",
    "        labels = np.fromfile(lbpath,\n",
    "                             dtype=np.uint8)\n",
    "\n",
    "    with open(images_path, 'rb') as imgpath:\n",
    "        magic, num, rows, cols = struct.unpack('>IIII',\n",
    "                                               imgpath.read(16))\n",
    "        images = np.fromfile(imgpath,\n",
    "                             dtype=np.uint8).reshape(len(labels), 784)\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = './datasets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Handwritten numbers training data\n",
    "\n",
    "X_train_nums,y_train_nums = load_data(path, kind='train')\n",
    "X_train = X_train_nums / 255\n",
    "\n",
    "X_test_nums, y_test_nums = load_data(path, kind='t10k')\n",
    "X_test_nums = X_test_nums / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_nums.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Random generate noise\n",
    "\n",
    "noise_random = np.random.rand(6000,784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15405 80839 10742 ... 97277  4278 13224]\n"
     ]
    }
   ],
   "source": [
    "# random take handwritten alphabet from Nist\n",
    "\n",
    "noise_letters,_ = load_data(path, kind='emnist-letters-train')\n",
    "\n",
    "idx_letters = np.random.randint(len(noise_letters), size=6000)\n",
    "print(idx_letters)\n",
    "noise_letters = noise_letters[idx_letters,:]\n",
    "\n",
    "noise_letters = noise_letters / 255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6000, 784)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise_letters.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x22b503d4518>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_test_nums[1].reshape(28,28),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x22b50424358>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(noise_random[1].reshape(28,28),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x22b50477908>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(noise_letters[1].reshape(28,28),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "noise_labels = np.full((6000), 10, dtype=int)\n",
    "# letters_labels = np.full((6000), 11, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x22b504c6e80>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train0 = X_train[y_train_nums == 0]\n",
    "y_train0 = y_train_nums[y_train_nums == 0]\n",
    "plt.imshow(X_train0[1].reshape(28,28),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11923, 784)\n",
      "(12742, 784)\n",
      "(11958, 784)\n",
      "(12131, 784)\n",
      "(11842, 784)\n",
      "(11421, 784)\n",
      "(11918, 784)\n",
      "(12265, 784)\n",
      "(11851, 784)\n",
      "(11949, 784)\n"
     ]
    }
   ],
   "source": [
    "X_train_split = []\n",
    "y_train_split = []\n",
    "num_classes = 12 \n",
    "\n",
    "for i in range(10):\n",
    "\n",
    "    index_i = np.where( y_train_nums == i )\n",
    "\n",
    "    X_train_i = X_train[index_i[0],:]\n",
    "    \n",
    "    y_train_i = y_train_nums[index_i ]\n",
    "    \n",
    "    noise_numbers = X_train\n",
    "    noise_numbers = np.delete( noise_numbers, index_i, axis = 0 )\n",
    "    \n",
    "    idx_noise_numbers = np.random.randint(len(noise_numbers), size=6000)\n",
    "#     print(idx_noise_numbers)\n",
    "    noise_numbers = noise_numbers[idx_noise_numbers,:]\n",
    "\n",
    "    X_train_i = np.concatenate((X_train_i, noise_numbers),axis=0)\n",
    "#     X_train_i= np.concatenate((X_train_i,noise_letters),axis=0)\n",
    "    print(X_train_i.shape)\n",
    "\n",
    "    y_train_i = np.concatenate((y_train_i, noise_labels),axis=0)\n",
    "#     y_train_i = np.concatenate((y_train_i, letters_labels),axis=0)\n",
    "    \n",
    "\n",
    "    \n",
    "    X_sparse_train = coo_matrix(X_train_i)\n",
    "    X_train_i, X_sparse_train, y_train_i = shuffle(X_train_i, X_sparse_train, y_train_i, random_state=42)\n",
    "\n",
    "    X_train_i = X_train_i.reshape(len(X_train_i),1,28,28).astype('float')\n",
    "    y_train_i = np_utils.to_categorical(y_train_i, num_classes)\n",
    "\n",
    "    X_train_split.append(X_train_i)\n",
    "    y_train_split.append(y_train_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11918, 1, 28, 28)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_split[6].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_split[9][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_split[9][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5923, 784)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[y_train_nums == 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11923"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train_split[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1, 28, 28)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 . Build Model -- keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import Adadelta\n",
    "\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def models_factory():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', \n",
    "                     data_format='channels_first',\n",
    "                     input_shape=(1, 28, 28)))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    # model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    # model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    # model.add(Dropout(0.5))\n",
    "    model.add(Dense(12, activation='softmax'))\n",
    "\n",
    "    model.compile(loss=categorical_crossentropy,\n",
    "                  optimizer=Adadelta(),\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 26, 26)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 30, 24, 32)        7520      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 15, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 13, 10, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 11, 8, 64)         36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                81984     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 12)                780       \n",
      "=================================================================\n",
      "Total params: 146,028\n",
      "Trainable params: 146,028\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = models_factory()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Train on 10730 samples, validate on 1193 samples\n",
      "Epoch 1/20\n",
      "10730/10730 [==============================] - 3s 312us/step - loss: 0.3022 - acc: 0.8943 - val_loss: 0.0648 - val_acc: 0.9732\n",
      "Epoch 2/20\n",
      "10730/10730 [==============================] - 2s 219us/step - loss: 0.0585 - acc: 0.9788 - val_loss: 0.0543 - val_acc: 0.9832\n",
      "Epoch 3/20\n",
      "10730/10730 [==============================] - 2s 221us/step - loss: 0.0405 - acc: 0.9874 - val_loss: 0.0342 - val_acc: 0.9891\n",
      "Epoch 4/20\n",
      "10730/10730 [==============================] - 2s 224us/step - loss: 0.0322 - acc: 0.9894 - val_loss: 0.0242 - val_acc: 0.9899\n",
      "Epoch 5/20\n",
      "10730/10730 [==============================] - 2s 222us/step - loss: 0.0262 - acc: 0.9915 - val_loss: 0.0178 - val_acc: 0.9933\n",
      "Epoch 6/20\n",
      "10730/10730 [==============================] - 2s 225us/step - loss: 0.0212 - acc: 0.9929 - val_loss: 0.0195 - val_acc: 0.9933\n",
      "Epoch 7/20\n",
      "10730/10730 [==============================] - 2s 222us/step - loss: 0.0163 - acc: 0.9946 - val_loss: 0.0139 - val_acc: 0.9950\n",
      "Epoch 8/20\n",
      "10730/10730 [==============================] - 2s 222us/step - loss: 0.0157 - acc: 0.9947 - val_loss: 0.0124 - val_acc: 0.9950\n",
      "Epoch 9/20\n",
      "10730/10730 [==============================] - 2s 224us/step - loss: 0.0108 - acc: 0.9961 - val_loss: 0.0183 - val_acc: 0.9941\n",
      "Epoch 10/20\n",
      "10730/10730 [==============================] - 2s 223us/step - loss: 0.0089 - acc: 0.9966 - val_loss: 0.0144 - val_acc: 0.9958\n",
      "Epoch 11/20\n",
      "10730/10730 [==============================] - 2s 223us/step - loss: 0.0084 - acc: 0.9973 - val_loss: 0.0108 - val_acc: 0.9958\n",
      "Epoch 12/20\n",
      "10730/10730 [==============================] - 2s 225us/step - loss: 0.0086 - acc: 0.9967 - val_loss: 0.0177 - val_acc: 0.9941\n",
      "Epoch 13/20\n",
      "10730/10730 [==============================] - 2s 225us/step - loss: 0.0058 - acc: 0.9979 - val_loss: 0.0390 - val_acc: 0.9883\n",
      "Epoch 14/20\n",
      "10730/10730 [==============================] - 2s 222us/step - loss: 0.0052 - acc: 0.9982 - val_loss: 0.0145 - val_acc: 0.9941\n",
      "Epoch 15/20\n",
      "10730/10730 [==============================] - 2s 221us/step - loss: 0.0047 - acc: 0.9984 - val_loss: 0.0158 - val_acc: 0.9958\n",
      "Epoch 16/20\n",
      "10730/10730 [==============================] - 2s 221us/step - loss: 0.0033 - acc: 0.9988 - val_loss: 0.0116 - val_acc: 0.9958\n",
      "Epoch 17/20\n",
      "10730/10730 [==============================] - 2s 220us/step - loss: 0.0041 - acc: 0.9983 - val_loss: 0.0149 - val_acc: 0.9966\n",
      "Epoch 18/20\n",
      "10730/10730 [==============================] - 2s 220us/step - loss: 0.0026 - acc: 0.9993 - val_loss: 0.0162 - val_acc: 0.9958\n",
      "Epoch 19/20\n",
      "10730/10730 [==============================] - 2s 220us/step - loss: 0.0025 - acc: 0.9993 - val_loss: 0.0227 - val_acc: 0.9950\n",
      "Epoch 20/20\n",
      "10730/10730 [==============================] - 2s 220us/step - loss: 0.0015 - acc: 0.9994 - val_loss: 0.0184 - val_acc: 0.9958\n",
      "1\n",
      "Train on 11467 samples, validate on 1275 samples\n",
      "Epoch 1/20\n",
      "11467/11467 [==============================] - 3s 286us/step - loss: 0.3214 - acc: 0.8814 - val_loss: 0.0422 - val_acc: 0.9859\n",
      "Epoch 2/20\n",
      "11467/11467 [==============================] - 3s 222us/step - loss: 0.0633 - acc: 0.9794 - val_loss: 0.0372 - val_acc: 0.9890\n",
      "Epoch 3/20\n",
      "11467/11467 [==============================] - 3s 223us/step - loss: 0.0478 - acc: 0.9859 - val_loss: 0.0443 - val_acc: 0.9859\n",
      "Epoch 4/20\n",
      "11467/11467 [==============================] - 3s 221us/step - loss: 0.0410 - acc: 0.9867 - val_loss: 0.0175 - val_acc: 0.9945\n",
      "Epoch 5/20\n",
      "11467/11467 [==============================] - 3s 220us/step - loss: 0.0285 - acc: 0.9904 - val_loss: 0.0150 - val_acc: 0.9953\n",
      "Epoch 6/20\n",
      "11467/11467 [==============================] - 3s 222us/step - loss: 0.0203 - acc: 0.9928 - val_loss: 0.0135 - val_acc: 0.9976\n",
      "Epoch 7/20\n",
      "11467/11467 [==============================] - 3s 220us/step - loss: 0.0184 - acc: 0.9939 - val_loss: 0.0175 - val_acc: 0.9945\n",
      "Epoch 8/20\n",
      "11467/11467 [==============================] - 3s 219us/step - loss: 0.0163 - acc: 0.9948 - val_loss: 0.0167 - val_acc: 0.9945\n",
      "Epoch 9/20\n",
      "11467/11467 [==============================] - 3s 220us/step - loss: 0.0139 - acc: 0.9957 - val_loss: 0.0119 - val_acc: 0.9969\n",
      "Epoch 10/20\n",
      "11467/11467 [==============================] - 3s 220us/step - loss: 0.0133 - acc: 0.9954 - val_loss: 0.0095 - val_acc: 0.9976\n",
      "Epoch 11/20\n",
      "11467/11467 [==============================] - 3s 219us/step - loss: 0.0127 - acc: 0.9960 - val_loss: 0.0089 - val_acc: 0.9976\n",
      "Epoch 12/20\n",
      "11467/11467 [==============================] - 3s 220us/step - loss: 0.0092 - acc: 0.9963 - val_loss: 0.0091 - val_acc: 0.9976\n",
      "Epoch 13/20\n",
      "11467/11467 [==============================] - 3s 220us/step - loss: 0.0081 - acc: 0.9975 - val_loss: 0.0084 - val_acc: 0.9984\n",
      "Epoch 14/20\n",
      "11467/11467 [==============================] - 3s 221us/step - loss: 0.0083 - acc: 0.9962 - val_loss: 0.0082 - val_acc: 0.9976\n",
      "Epoch 15/20\n",
      "11467/11467 [==============================] - 3s 221us/step - loss: 0.0066 - acc: 0.9980 - val_loss: 0.0057 - val_acc: 0.9984\n",
      "Epoch 16/20\n",
      "11467/11467 [==============================] - 3s 220us/step - loss: 0.0055 - acc: 0.9984 - val_loss: 0.0073 - val_acc: 0.9984\n",
      "Epoch 17/20\n",
      "11467/11467 [==============================] - 3s 220us/step - loss: 0.0050 - acc: 0.9986 - val_loss: 0.0085 - val_acc: 0.9969\n",
      "Epoch 18/20\n",
      "11467/11467 [==============================] - 3s 220us/step - loss: 0.0042 - acc: 0.9985 - val_loss: 0.0105 - val_acc: 0.9984\n",
      "Epoch 19/20\n",
      "11467/11467 [==============================] - 3s 221us/step - loss: 0.0056 - acc: 0.9979 - val_loss: 0.0151 - val_acc: 0.9969\n",
      "Epoch 20/20\n",
      "11467/11467 [==============================] - 3s 219us/step - loss: 0.0033 - acc: 0.9990 - val_loss: 0.0128 - val_acc: 0.9969\n",
      "2\n",
      "Train on 10762 samples, validate on 1196 samples\n",
      "Epoch 1/20\n",
      "10762/10762 [==============================] - 3s 278us/step - loss: 0.4496 - acc: 0.8025 - val_loss: 0.2758 - val_acc: 0.8863\n",
      "Epoch 2/20\n",
      "10762/10762 [==============================] - 2s 222us/step - loss: 0.1250 - acc: 0.9547 - val_loss: 0.0851 - val_acc: 0.9649\n",
      "Epoch 3/20\n",
      "10762/10762 [==============================] - 2s 221us/step - loss: 0.0793 - acc: 0.9736 - val_loss: 0.0556 - val_acc: 0.9816\n",
      "Epoch 4/20\n",
      "10762/10762 [==============================] - 2s 223us/step - loss: 0.0544 - acc: 0.9816 - val_loss: 0.0486 - val_acc: 0.9808\n",
      "Epoch 5/20\n",
      "10762/10762 [==============================] - 2s 222us/step - loss: 0.0464 - acc: 0.9837 - val_loss: 0.0432 - val_acc: 0.9849\n",
      "Epoch 6/20\n",
      "10762/10762 [==============================] - 2s 223us/step - loss: 0.0391 - acc: 0.9861 - val_loss: 0.0275 - val_acc: 0.9933\n",
      "Epoch 7/20\n",
      "10762/10762 [==============================] - 2s 221us/step - loss: 0.0310 - acc: 0.9890 - val_loss: 0.0594 - val_acc: 0.9799\n",
      "Epoch 8/20\n",
      "10762/10762 [==============================] - 2s 220us/step - loss: 0.0280 - acc: 0.9902 - val_loss: 0.0343 - val_acc: 0.9858\n",
      "Epoch 9/20\n",
      "10762/10762 [==============================] - 2s 220us/step - loss: 0.0220 - acc: 0.9922 - val_loss: 0.0241 - val_acc: 0.9925\n",
      "Epoch 10/20\n",
      "10762/10762 [==============================] - 2s 220us/step - loss: 0.0171 - acc: 0.9938 - val_loss: 0.0240 - val_acc: 0.9941\n",
      "Epoch 11/20\n",
      "10762/10762 [==============================] - 2s 220us/step - loss: 0.0142 - acc: 0.9953 - val_loss: 0.0152 - val_acc: 0.9933\n",
      "Epoch 12/20\n",
      "10762/10762 [==============================] - 2s 221us/step - loss: 0.0130 - acc: 0.9948 - val_loss: 0.0279 - val_acc: 0.9916\n",
      "Epoch 13/20\n",
      "10762/10762 [==============================] - 2s 220us/step - loss: 0.0095 - acc: 0.9968 - val_loss: 0.0218 - val_acc: 0.9950\n",
      "Epoch 14/20\n",
      "10762/10762 [==============================] - 2s 222us/step - loss: 0.0089 - acc: 0.9966 - val_loss: 0.0175 - val_acc: 0.9950\n",
      "Epoch 15/20\n",
      "10762/10762 [==============================] - 2s 222us/step - loss: 0.0049 - acc: 0.9986 - val_loss: 0.0192 - val_acc: 0.9958\n",
      "Epoch 16/20\n",
      "10762/10762 [==============================] - 2s 221us/step - loss: 0.0076 - acc: 0.9979 - val_loss: 0.0137 - val_acc: 0.9967\n",
      "Epoch 17/20\n",
      "10762/10762 [==============================] - 2s 221us/step - loss: 0.0051 - acc: 0.9980 - val_loss: 0.0166 - val_acc: 0.9958\n",
      "Epoch 18/20\n",
      "10762/10762 [==============================] - 2s 221us/step - loss: 0.0038 - acc: 0.9988 - val_loss: 0.0162 - val_acc: 0.9950\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10762/10762 [==============================] - 2s 223us/step - loss: 0.0066 - acc: 0.9980 - val_loss: 0.0159 - val_acc: 0.9950\n",
      "Epoch 20/20\n",
      "10762/10762 [==============================] - 2s 222us/step - loss: 0.0030 - acc: 0.9989 - val_loss: 0.0184 - val_acc: 0.9967\n",
      "3\n",
      "Train on 10917 samples, validate on 1214 samples\n",
      "Epoch 1/20\n",
      "10917/10917 [==============================] - 3s 288us/step - loss: 0.4896 - acc: 0.7867 - val_loss: 0.2605 - val_acc: 0.9028\n",
      "Epoch 2/20\n",
      "10917/10917 [==============================] - 2s 220us/step - loss: 0.1588 - acc: 0.9437 - val_loss: 0.1113 - val_acc: 0.9654\n",
      "Epoch 3/20\n",
      "10917/10917 [==============================] - 2s 220us/step - loss: 0.1021 - acc: 0.9655 - val_loss: 0.0823 - val_acc: 0.9703\n",
      "Epoch 4/20\n",
      "10917/10917 [==============================] - 2s 220us/step - loss: 0.0712 - acc: 0.9734 - val_loss: 0.0571 - val_acc: 0.9843\n",
      "Epoch 5/20\n",
      "10917/10917 [==============================] - 2s 221us/step - loss: 0.0478 - acc: 0.9836 - val_loss: 0.0351 - val_acc: 0.9876\n",
      "Epoch 6/20\n",
      "10917/10917 [==============================] - 2s 223us/step - loss: 0.0382 - acc: 0.9874 - val_loss: 0.0411 - val_acc: 0.9860\n",
      "Epoch 7/20\n",
      "10917/10917 [==============================] - 2s 223us/step - loss: 0.0280 - acc: 0.9903 - val_loss: 0.0189 - val_acc: 0.9942\n",
      "Epoch 8/20\n",
      "10917/10917 [==============================] - 2s 220us/step - loss: 0.0252 - acc: 0.9909 - val_loss: 0.0275 - val_acc: 0.9909\n",
      "Epoch 9/20\n",
      "10917/10917 [==============================] - 2s 221us/step - loss: 0.0204 - acc: 0.9920 - val_loss: 0.0777 - val_acc: 0.9712\n",
      "Epoch 10/20\n",
      "10917/10917 [==============================] - 2s 221us/step - loss: 0.0154 - acc: 0.9942 - val_loss: 0.0289 - val_acc: 0.9876\n",
      "Epoch 11/20\n",
      "10917/10917 [==============================] - 2s 223us/step - loss: 0.0127 - acc: 0.9957 - val_loss: 0.0221 - val_acc: 0.9926\n",
      "Epoch 12/20\n",
      "10917/10917 [==============================] - 2s 225us/step - loss: 0.0086 - acc: 0.9977 - val_loss: 0.0167 - val_acc: 0.9951\n",
      "Epoch 13/20\n",
      "10917/10917 [==============================] - 2s 223us/step - loss: 0.0099 - acc: 0.9962 - val_loss: 0.0149 - val_acc: 0.9926\n",
      "Epoch 14/20\n",
      "10917/10917 [==============================] - 2s 223us/step - loss: 0.0086 - acc: 0.9975 - val_loss: 0.0182 - val_acc: 0.9926\n",
      "Epoch 15/20\n",
      "10917/10917 [==============================] - 2s 225us/step - loss: 0.0065 - acc: 0.9978 - val_loss: 0.0175 - val_acc: 0.9934\n",
      "Epoch 16/20\n",
      "10917/10917 [==============================] - 2s 221us/step - loss: 0.0027 - acc: 0.9995 - val_loss: 0.0153 - val_acc: 0.9942\n",
      "Epoch 17/20\n",
      "10917/10917 [==============================] - 2s 222us/step - loss: 0.0040 - acc: 0.9991 - val_loss: 0.0178 - val_acc: 0.9951\n",
      "Epoch 18/20\n",
      "10917/10917 [==============================] - 2s 220us/step - loss: 0.0035 - acc: 0.9987 - val_loss: 0.0140 - val_acc: 0.9934\n",
      "Epoch 19/20\n",
      "10917/10917 [==============================] - 2s 223us/step - loss: 0.0024 - acc: 0.9995 - val_loss: 0.0263 - val_acc: 0.9934\n",
      "Epoch 20/20\n",
      "10917/10917 [==============================] - 2s 221us/step - loss: 0.0021 - acc: 0.9995 - val_loss: 0.0188 - val_acc: 0.9942\n",
      "4\n",
      "Train on 10657 samples, validate on 1185 samples\n",
      "Epoch 1/20\n",
      "10657/10657 [==============================] - 3s 301us/step - loss: 0.3567 - acc: 0.8650 - val_loss: 0.0878 - val_acc: 0.9688\n",
      "Epoch 2/20\n",
      "10657/10657 [==============================] - 2s 222us/step - loss: 0.0746 - acc: 0.9765 - val_loss: 0.1234 - val_acc: 0.9578\n",
      "Epoch 3/20\n",
      "10657/10657 [==============================] - 2s 223us/step - loss: 0.0524 - acc: 0.9830 - val_loss: 0.0627 - val_acc: 0.9806\n",
      "Epoch 4/20\n",
      "10657/10657 [==============================] - 2s 222us/step - loss: 0.0411 - acc: 0.9876 - val_loss: 0.0233 - val_acc: 0.9916\n",
      "Epoch 5/20\n",
      "10657/10657 [==============================] - 2s 221us/step - loss: 0.0343 - acc: 0.9895 - val_loss: 0.0250 - val_acc: 0.9899\n",
      "Epoch 6/20\n",
      "10657/10657 [==============================] - 2s 222us/step - loss: 0.0285 - acc: 0.9899 - val_loss: 0.1558 - val_acc: 0.9384\n",
      "Epoch 7/20\n",
      "10657/10657 [==============================] - 2s 223us/step - loss: 0.0221 - acc: 0.9931 - val_loss: 0.0689 - val_acc: 0.9738\n",
      "Epoch 8/20\n",
      "10657/10657 [==============================] - 2s 222us/step - loss: 0.0210 - acc: 0.9934 - val_loss: 0.0562 - val_acc: 0.9814\n",
      "Epoch 9/20\n",
      "10657/10657 [==============================] - 2s 224us/step - loss: 0.0169 - acc: 0.9949 - val_loss: 0.0281 - val_acc: 0.9924\n",
      "Epoch 10/20\n",
      "10657/10657 [==============================] - 2s 222us/step - loss: 0.0153 - acc: 0.9956 - val_loss: 0.0185 - val_acc: 0.9958\n",
      "Epoch 11/20\n",
      "10657/10657 [==============================] - 2s 221us/step - loss: 0.0131 - acc: 0.9962 - val_loss: 0.0220 - val_acc: 0.9932\n",
      "Epoch 12/20\n",
      "10657/10657 [==============================] - 2s 222us/step - loss: 0.0098 - acc: 0.9964 - val_loss: 0.0168 - val_acc: 0.9958\n",
      "Epoch 13/20\n",
      "10657/10657 [==============================] - 2s 220us/step - loss: 0.0098 - acc: 0.9968 - val_loss: 0.0202 - val_acc: 0.9949\n",
      "Epoch 14/20\n",
      "10657/10657 [==============================] - 2s 221us/step - loss: 0.0077 - acc: 0.9978 - val_loss: 0.0281 - val_acc: 0.9932\n",
      "Epoch 15/20\n",
      "10657/10657 [==============================] - 2s 222us/step - loss: 0.0070 - acc: 0.9980 - val_loss: 0.0199 - val_acc: 0.9958\n",
      "Epoch 16/20\n",
      "10657/10657 [==============================] - 2s 221us/step - loss: 0.0067 - acc: 0.9982 - val_loss: 0.0179 - val_acc: 0.9983\n",
      "Epoch 17/20\n",
      "10657/10657 [==============================] - 2s 224us/step - loss: 0.0035 - acc: 0.9994 - val_loss: 0.0178 - val_acc: 0.9958\n",
      "Epoch 18/20\n",
      "10657/10657 [==============================] - 2s 222us/step - loss: 0.0061 - acc: 0.9987 - val_loss: 0.0208 - val_acc: 0.9975\n",
      "Epoch 19/20\n",
      "10657/10657 [==============================] - 2s 221us/step - loss: 0.0032 - acc: 0.9993 - val_loss: 0.0199 - val_acc: 0.9975\n",
      "Epoch 20/20\n",
      "10657/10657 [==============================] - 2s 223us/step - loss: 0.0044 - acc: 0.9988 - val_loss: 0.0175 - val_acc: 0.9975\n",
      "5\n",
      "Train on 10278 samples, validate on 1143 samples\n",
      "Epoch 1/20\n",
      "10278/10278 [==============================] - 3s 290us/step - loss: 0.4202 - acc: 0.8283 - val_loss: 0.0962 - val_acc: 0.9711\n",
      "Epoch 2/20\n",
      "10278/10278 [==============================] - 2s 222us/step - loss: 0.0926 - acc: 0.9682 - val_loss: 0.0676 - val_acc: 0.9799\n",
      "Epoch 3/20\n",
      "10278/10278 [==============================] - 2s 222us/step - loss: 0.0559 - acc: 0.9806 - val_loss: 0.0949 - val_acc: 0.9668\n",
      "Epoch 4/20\n",
      "10278/10278 [==============================] - 2s 221us/step - loss: 0.0436 - acc: 0.9844 - val_loss: 0.0843 - val_acc: 0.9711\n",
      "Epoch 5/20\n",
      "10278/10278 [==============================] - 2s 221us/step - loss: 0.0378 - acc: 0.9886 - val_loss: 0.0670 - val_acc: 0.9808\n",
      "Epoch 6/20\n",
      "10278/10278 [==============================] - 2s 220us/step - loss: 0.0301 - acc: 0.9907 - val_loss: 0.0412 - val_acc: 0.9878\n",
      "Epoch 7/20\n",
      "10278/10278 [==============================] - 2s 221us/step - loss: 0.0262 - acc: 0.9910 - val_loss: 0.0353 - val_acc: 0.9895\n",
      "Epoch 8/20\n",
      "10278/10278 [==============================] - 2s 222us/step - loss: 0.0234 - acc: 0.9926 - val_loss: 0.0889 - val_acc: 0.9720\n",
      "Epoch 9/20\n",
      "10278/10278 [==============================] - 2s 222us/step - loss: 0.0177 - acc: 0.9935 - val_loss: 0.0387 - val_acc: 0.9913\n",
      "Epoch 10/20\n",
      "10278/10278 [==============================] - 2s 221us/step - loss: 0.0165 - acc: 0.9946 - val_loss: 0.0351 - val_acc: 0.9904\n",
      "Epoch 11/20\n",
      "10278/10278 [==============================] - 2s 224us/step - loss: 0.0142 - acc: 0.9943 - val_loss: 0.0396 - val_acc: 0.9913\n",
      "Epoch 12/20\n",
      "10278/10278 [==============================] - 2s 224us/step - loss: 0.0119 - acc: 0.9964 - val_loss: 0.0508 - val_acc: 0.9843\n",
      "Epoch 13/20\n",
      "10278/10278 [==============================] - 2s 221us/step - loss: 0.0109 - acc: 0.9956 - val_loss: 0.0410 - val_acc: 0.9921\n",
      "Epoch 14/20\n",
      "10278/10278 [==============================] - 2s 221us/step - loss: 0.0109 - acc: 0.9954 - val_loss: 0.0615 - val_acc: 0.9860\n",
      "Epoch 15/20\n",
      "10278/10278 [==============================] - 2s 221us/step - loss: 0.0075 - acc: 0.9972 - val_loss: 0.0336 - val_acc: 0.9921\n",
      "Epoch 16/20\n",
      "10278/10278 [==============================] - 2s 220us/step - loss: 0.0068 - acc: 0.9975 - val_loss: 0.0430 - val_acc: 0.9895\n",
      "Epoch 17/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10278/10278 [==============================] - 2s 221us/step - loss: 0.0055 - acc: 0.9983 - val_loss: 0.0869 - val_acc: 0.9764\n",
      "Epoch 18/20\n",
      "10278/10278 [==============================] - 2s 222us/step - loss: 0.0051 - acc: 0.9982 - val_loss: 0.0395 - val_acc: 0.9904\n",
      "Epoch 19/20\n",
      "10278/10278 [==============================] - 2s 223us/step - loss: 0.0034 - acc: 0.9988 - val_loss: 0.0424 - val_acc: 0.9904\n",
      "Epoch 20/20\n",
      "10278/10278 [==============================] - 2s 225us/step - loss: 0.0042 - acc: 0.9983 - val_loss: 0.0413 - val_acc: 0.9895\n",
      "6\n",
      "Train on 10726 samples, validate on 1192 samples\n",
      "Epoch 1/20\n",
      "10726/10726 [==============================] - 3s 308us/step - loss: 0.4116 - acc: 0.8536 - val_loss: 0.1032 - val_acc: 0.9581\n",
      "Epoch 2/20\n",
      "10726/10726 [==============================] - 2s 221us/step - loss: 0.0713 - acc: 0.9776 - val_loss: 0.0557 - val_acc: 0.9799\n",
      "Epoch 3/20\n",
      "10726/10726 [==============================] - 2s 221us/step - loss: 0.0418 - acc: 0.9846 - val_loss: 0.0437 - val_acc: 0.9849\n",
      "Epoch 4/20\n",
      "10726/10726 [==============================] - 2s 224us/step - loss: 0.0315 - acc: 0.9893 - val_loss: 0.0312 - val_acc: 0.9924\n",
      "Epoch 5/20\n",
      "10726/10726 [==============================] - 2s 221us/step - loss: 0.0237 - acc: 0.9922 - val_loss: 0.0300 - val_acc: 0.9891\n",
      "Epoch 6/20\n",
      "10726/10726 [==============================] - 2s 221us/step - loss: 0.0226 - acc: 0.9916 - val_loss: 0.0255 - val_acc: 0.9924\n",
      "Epoch 7/20\n",
      "10726/10726 [==============================] - 2s 221us/step - loss: 0.0139 - acc: 0.9957 - val_loss: 0.0210 - val_acc: 0.9933\n",
      "Epoch 8/20\n",
      "10726/10726 [==============================] - 2s 222us/step - loss: 0.0123 - acc: 0.9958 - val_loss: 0.0285 - val_acc: 0.9908\n",
      "Epoch 9/20\n",
      "10726/10726 [==============================] - 2s 225us/step - loss: 0.0110 - acc: 0.9962 - val_loss: 0.0333 - val_acc: 0.9908\n",
      "Epoch 10/20\n",
      "10726/10726 [==============================] - 2s 221us/step - loss: 0.0118 - acc: 0.9961 - val_loss: 0.0168 - val_acc: 0.9958\n",
      "Epoch 11/20\n",
      "10726/10726 [==============================] - 2s 222us/step - loss: 0.0072 - acc: 0.9979 - val_loss: 0.0189 - val_acc: 0.9950\n",
      "Epoch 12/20\n",
      "10726/10726 [==============================] - 2s 220us/step - loss: 0.0066 - acc: 0.9976 - val_loss: 0.0245 - val_acc: 0.9924\n",
      "Epoch 13/20\n",
      "10726/10726 [==============================] - 2s 222us/step - loss: 0.0049 - acc: 0.9985 - val_loss: 0.0192 - val_acc: 0.9958\n",
      "Epoch 14/20\n",
      "10726/10726 [==============================] - 2s 221us/step - loss: 0.0037 - acc: 0.9986 - val_loss: 0.0244 - val_acc: 0.9924\n",
      "Epoch 15/20\n",
      "10726/10726 [==============================] - 2s 221us/step - loss: 0.0039 - acc: 0.9984 - val_loss: 0.0265 - val_acc: 0.9933\n",
      "Epoch 16/20\n",
      "10726/10726 [==============================] - 2s 223us/step - loss: 0.0029 - acc: 0.9988 - val_loss: 0.0336 - val_acc: 0.9908\n",
      "Epoch 17/20\n",
      "10726/10726 [==============================] - 2s 225us/step - loss: 0.0039 - acc: 0.9983 - val_loss: 0.0186 - val_acc: 0.9950\n",
      "Epoch 18/20\n",
      "10726/10726 [==============================] - 2s 223us/step - loss: 0.0016 - acc: 0.9995 - val_loss: 0.0186 - val_acc: 0.9966\n",
      "Epoch 19/20\n",
      "10726/10726 [==============================] - 2s 222us/step - loss: 0.0029 - acc: 0.9994 - val_loss: 0.0220 - val_acc: 0.9941\n",
      "Epoch 20/20\n",
      "10726/10726 [==============================] - 2s 222us/step - loss: 8.2972e-04 - acc: 0.9998 - val_loss: 0.0177 - val_acc: 0.9966\n",
      "7\n",
      "Train on 11038 samples, validate on 1227 samples\n",
      "Epoch 1/20\n",
      "11038/11038 [==============================] - 3s 310us/step - loss: 0.3666 - acc: 0.8685 - val_loss: 0.1405 - val_acc: 0.9470\n",
      "Epoch 2/20\n",
      "11038/11038 [==============================] - 2s 223us/step - loss: 0.0750 - acc: 0.9747 - val_loss: 0.0770 - val_acc: 0.9764\n",
      "Epoch 3/20\n",
      "11038/11038 [==============================] - 2s 222us/step - loss: 0.0542 - acc: 0.9823 - val_loss: 0.0968 - val_acc: 0.9698\n",
      "Epoch 4/20\n",
      "11038/11038 [==============================] - 2s 222us/step - loss: 0.0447 - acc: 0.9858 - val_loss: 0.0861 - val_acc: 0.9747\n",
      "Epoch 5/20\n",
      "11038/11038 [==============================] - 2s 223us/step - loss: 0.0334 - acc: 0.9889 - val_loss: 0.0465 - val_acc: 0.9845\n",
      "Epoch 6/20\n",
      "11038/11038 [==============================] - 2s 222us/step - loss: 0.0302 - acc: 0.9913 - val_loss: 0.0860 - val_acc: 0.9756\n",
      "Epoch 7/20\n",
      "11038/11038 [==============================] - 2s 222us/step - loss: 0.0255 - acc: 0.9925 - val_loss: 0.0967 - val_acc: 0.9723\n",
      "Epoch 8/20\n",
      "11038/11038 [==============================] - 2s 224us/step - loss: 0.0215 - acc: 0.9933 - val_loss: 0.0739 - val_acc: 0.9813\n",
      "Epoch 9/20\n",
      "11038/11038 [==============================] - 2s 225us/step - loss: 0.0181 - acc: 0.9939 - val_loss: 0.0397 - val_acc: 0.9861\n",
      "Epoch 10/20\n",
      "11038/11038 [==============================] - 2s 223us/step - loss: 0.0167 - acc: 0.9949 - val_loss: 0.0400 - val_acc: 0.9878\n",
      "Epoch 11/20\n",
      "11038/11038 [==============================] - 2s 223us/step - loss: 0.0126 - acc: 0.9960 - val_loss: 0.0421 - val_acc: 0.9878\n",
      "Epoch 12/20\n",
      "11038/11038 [==============================] - 2s 223us/step - loss: 0.0116 - acc: 0.9967 - val_loss: 0.0352 - val_acc: 0.9886\n",
      "Epoch 13/20\n",
      "11038/11038 [==============================] - 2s 222us/step - loss: 0.0114 - acc: 0.9957 - val_loss: 0.0395 - val_acc: 0.9870\n",
      "Epoch 14/20\n",
      "11038/11038 [==============================] - 2s 223us/step - loss: 0.0090 - acc: 0.9975 - val_loss: 0.0447 - val_acc: 0.9870\n",
      "Epoch 15/20\n",
      "11038/11038 [==============================] - 2s 222us/step - loss: 0.0073 - acc: 0.9974 - val_loss: 0.0396 - val_acc: 0.9902\n",
      "Epoch 16/20\n",
      "11038/11038 [==============================] - 2s 223us/step - loss: 0.0066 - acc: 0.9975 - val_loss: 0.0268 - val_acc: 0.9935\n",
      "Epoch 17/20\n",
      "11038/11038 [==============================] - 2s 223us/step - loss: 0.0047 - acc: 0.9986 - val_loss: 0.0385 - val_acc: 0.9894\n",
      "Epoch 18/20\n",
      "11038/11038 [==============================] - 2s 221us/step - loss: 0.0038 - acc: 0.9991 - val_loss: 0.0305 - val_acc: 0.9919\n",
      "Epoch 19/20\n",
      "11038/11038 [==============================] - 2s 222us/step - loss: 0.0037 - acc: 0.9987 - val_loss: 0.0367 - val_acc: 0.9902\n",
      "Epoch 20/20\n",
      "11038/11038 [==============================] - 2s 223us/step - loss: 0.0024 - acc: 0.9993 - val_loss: 0.0349 - val_acc: 0.9910\n",
      "8\n",
      "Train on 10665 samples, validate on 1186 samples\n",
      "Epoch 1/20\n",
      "10665/10665 [==============================] - 3s 309us/step - loss: 0.5870 - acc: 0.7240 - val_loss: 0.2939 - val_acc: 0.8676\n",
      "Epoch 2/20\n",
      "10665/10665 [==============================] - 2s 222us/step - loss: 0.1680 - acc: 0.9388 - val_loss: 0.1174 - val_acc: 0.9519\n",
      "Epoch 3/20\n",
      "10665/10665 [==============================] - 2s 222us/step - loss: 0.0868 - acc: 0.9707 - val_loss: 0.1835 - val_acc: 0.9376\n",
      "Epoch 4/20\n",
      "10665/10665 [==============================] - 2s 224us/step - loss: 0.0613 - acc: 0.9787 - val_loss: 0.0815 - val_acc: 0.9688\n",
      "Epoch 5/20\n",
      "10665/10665 [==============================] - 2s 225us/step - loss: 0.0456 - acc: 0.9843 - val_loss: 0.2148 - val_acc: 0.9351\n",
      "Epoch 6/20\n",
      "10665/10665 [==============================] - 2s 222us/step - loss: 0.0329 - acc: 0.9898 - val_loss: 0.0319 - val_acc: 0.9865\n",
      "Epoch 7/20\n",
      "10665/10665 [==============================] - 2s 223us/step - loss: 0.0294 - acc: 0.9899 - val_loss: 0.0281 - val_acc: 0.9916\n",
      "Epoch 8/20\n",
      "10665/10665 [==============================] - 2s 224us/step - loss: 0.0204 - acc: 0.9925 - val_loss: 0.0314 - val_acc: 0.9899\n",
      "Epoch 9/20\n",
      "10665/10665 [==============================] - 2s 224us/step - loss: 0.0165 - acc: 0.9941 - val_loss: 0.0277 - val_acc: 0.9890\n",
      "Epoch 10/20\n",
      "10665/10665 [==============================] - 2s 223us/step - loss: 0.0138 - acc: 0.9958 - val_loss: 0.0700 - val_acc: 0.9806\n",
      "Epoch 11/20\n",
      "10665/10665 [==============================] - 2s 223us/step - loss: 0.0094 - acc: 0.9964 - val_loss: 0.0323 - val_acc: 0.9882\n",
      "Epoch 12/20\n",
      "10665/10665 [==============================] - 2s 222us/step - loss: 0.0106 - acc: 0.9967 - val_loss: 0.0668 - val_acc: 0.9823\n",
      "Epoch 13/20\n",
      "10665/10665 [==============================] - 2s 222us/step - loss: 0.0055 - acc: 0.9985 - val_loss: 0.0583 - val_acc: 0.9865\n",
      "Epoch 14/20\n",
      "10665/10665 [==============================] - 2s 222us/step - loss: 0.0060 - acc: 0.9977 - val_loss: 0.0334 - val_acc: 0.9924\n",
      "Epoch 15/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10665/10665 [==============================] - 2s 221us/step - loss: 0.0027 - acc: 0.9992 - val_loss: 0.0489 - val_acc: 0.9899\n",
      "Epoch 16/20\n",
      "10665/10665 [==============================] - 2s 221us/step - loss: 0.0044 - acc: 0.9985 - val_loss: 0.0458 - val_acc: 0.9874\n",
      "Epoch 17/20\n",
      "10665/10665 [==============================] - 2s 222us/step - loss: 0.0025 - acc: 0.9992 - val_loss: 0.0660 - val_acc: 0.9865\n",
      "Epoch 18/20\n",
      "10665/10665 [==============================] - 2s 223us/step - loss: 0.0038 - acc: 0.9987 - val_loss: 0.0315 - val_acc: 0.9924\n",
      "Epoch 19/20\n",
      "10665/10665 [==============================] - 2s 221us/step - loss: 0.0012 - acc: 0.9996 - val_loss: 0.0435 - val_acc: 0.9882\n",
      "Epoch 20/20\n",
      "10665/10665 [==============================] - 2s 226us/step - loss: 0.0013 - acc: 0.9998 - val_loss: 0.0456 - val_acc: 0.9890\n",
      "9\n",
      "Train on 10754 samples, validate on 1195 samples\n",
      "Epoch 1/20\n",
      "10754/10754 [==============================] - 3s 303us/step - loss: 0.5665 - acc: 0.7648 - val_loss: 0.9184 - val_acc: 0.5138\n",
      "Epoch 2/20\n",
      "10754/10754 [==============================] - 2s 221us/step - loss: 0.1914 - acc: 0.9244 - val_loss: 0.1144 - val_acc: 0.9582\n",
      "Epoch 3/20\n",
      "10754/10754 [==============================] - 2s 222us/step - loss: 0.1045 - acc: 0.9633 - val_loss: 0.0888 - val_acc: 0.9682\n",
      "Epoch 4/20\n",
      "10754/10754 [==============================] - 2s 223us/step - loss: 0.0824 - acc: 0.9709 - val_loss: 0.0589 - val_acc: 0.9799\n",
      "Epoch 5/20\n",
      "10754/10754 [==============================] - 2s 221us/step - loss: 0.0523 - acc: 0.9813 - val_loss: 0.0447 - val_acc: 0.9874\n",
      "Epoch 6/20\n",
      "10754/10754 [==============================] - 2s 221us/step - loss: 0.0490 - acc: 0.9833 - val_loss: 0.1309 - val_acc: 0.9556\n",
      "Epoch 7/20\n",
      "10754/10754 [==============================] - 2s 222us/step - loss: 0.0423 - acc: 0.9857 - val_loss: 0.0395 - val_acc: 0.9833\n",
      "Epoch 8/20\n",
      "10754/10754 [==============================] - 2s 223us/step - loss: 0.0337 - acc: 0.9890 - val_loss: 0.0439 - val_acc: 0.9866\n",
      "Epoch 9/20\n",
      "10754/10754 [==============================] - 2s 222us/step - loss: 0.0284 - acc: 0.9907 - val_loss: 0.0438 - val_acc: 0.9841\n",
      "Epoch 10/20\n",
      "10754/10754 [==============================] - 2s 221us/step - loss: 0.0240 - acc: 0.9920 - val_loss: 0.0372 - val_acc: 0.9849\n",
      "Epoch 11/20\n",
      "10754/10754 [==============================] - 2s 220us/step - loss: 0.0205 - acc: 0.9927 - val_loss: 0.0343 - val_acc: 0.9883\n",
      "Epoch 12/20\n",
      "10754/10754 [==============================] - 2s 221us/step - loss: 0.0178 - acc: 0.9942 - val_loss: 0.0369 - val_acc: 0.9891\n",
      "Epoch 13/20\n",
      "10754/10754 [==============================] - 2s 221us/step - loss: 0.0154 - acc: 0.9950 - val_loss: 0.0395 - val_acc: 0.9874\n",
      "Epoch 14/20\n",
      "10754/10754 [==============================] - 2s 221us/step - loss: 0.0157 - acc: 0.9934 - val_loss: 0.0453 - val_acc: 0.9874\n",
      "Epoch 15/20\n",
      "10754/10754 [==============================] - 2s 221us/step - loss: 0.0101 - acc: 0.9965 - val_loss: 0.0407 - val_acc: 0.9891\n",
      "Epoch 16/20\n",
      "10754/10754 [==============================] - 2s 222us/step - loss: 0.0071 - acc: 0.9981 - val_loss: 0.0423 - val_acc: 0.9874\n",
      "Epoch 17/20\n",
      "10754/10754 [==============================] - 2s 221us/step - loss: 0.0104 - acc: 0.9960 - val_loss: 0.0308 - val_acc: 0.9900\n",
      "Epoch 18/20\n",
      "10754/10754 [==============================] - 2s 223us/step - loss: 0.0066 - acc: 0.9978 - val_loss: 0.0331 - val_acc: 0.9908\n",
      "Epoch 19/20\n",
      "10754/10754 [==============================] - 2s 221us/step - loss: 0.0071 - acc: 0.9980 - val_loss: 0.0301 - val_acc: 0.9900\n",
      "Epoch 20/20\n",
      "10754/10754 [==============================] - 2s 221us/step - loss: 0.0031 - acc: 0.9994 - val_loss: 0.0357 - val_acc: 0.9883\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,r2_score\n",
    "accs = []\n",
    "history_models = []\n",
    "models_weights = []\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    print(i)\n",
    "    model = models_factory()\n",
    "\n",
    "    X_train=X_train_split[i]\n",
    "    y_train = y_train_split[i]\n",
    "    \n",
    "\n",
    "    history = model.fit(X_train, y_train, epochs=20, batch_size=128, validation_split = 0.1)\n",
    "    \n",
    "    weights = model.get_weights()\n",
    "    history_models.append(history)\n",
    "    models_weights.append(weights)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(models_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    model = models_factory()\n",
    "    model.set_weights(models_weights[i])\n",
    "    model.save_weights('./models/magic/my_model_weights' + str(i) + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Your model needs be improved (change parameters)\n",
    "2. You may need to try a different machine learning algorithm (not all algorithms created equal)\n",
    "3. You need more data (subtle relationship difficult to find)\n",
    "4. You may need to try transforming your data (dependent upon algorithm used)\n",
    "5. There may be no relationship between your dependent and independent variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 . Evaluate & Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Testing set (mnist testing set)\n",
    "X_test = X_test_nums\n",
    "y_test = y_test_nums\n",
    "\n",
    "X_test  = X_test.reshape(len(X_test),1,28,28).astype('float')\n",
    "# y_test  = np_utils.to_categorical(y_test,  num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "y_test.shape   = \n",
      "(8987,)\n",
      "1\n",
      "y_test.shape   = \n",
      "(7795,)\n",
      "2\n",
      "y_test.shape   = \n",
      "(6708,)\n",
      "3\n",
      "y_test.shape   = \n",
      "(5699,)\n",
      "4\n",
      "y_test.shape   = \n",
      "(4713,)\n",
      "5\n",
      "y_test.shape   = \n",
      "(3807,)\n",
      "6\n",
      "y_test.shape   = \n",
      "(2892,)\n",
      "7\n",
      "y_test.shape   = \n",
      "(1907,)\n",
      "8\n",
      "y_test.shape   = \n",
      "(980,)\n",
      "9\n",
      "y_test.shape   = \n",
      "(25,)\n"
     ]
    }
   ],
   "source": [
    "y_true = []\n",
    "for i in range(10):\n",
    "\n",
    "    print( i )\n",
    "    model = models_factory()\n",
    "    model.load_weights('./models/magic/my_model_weights' + str(i) + '.h5')\n",
    "    \n",
    "    y_pred_i = model.predict( X_test, verbose= 0 )\n",
    "    \n",
    "#     print(y_pred_i.shape )\n",
    "    \n",
    "    y_pred_i_int = [np.around(x) for x in y_pred_i]\n",
    "    y_pred_i_max = np.argmax(y_pred_i_int, axis=1)\n",
    "\n",
    "#     take out index that predict = i \n",
    "#     print(y_pred_i_max.shape )\n",
    "    \n",
    "    index_i = np.where(y_pred_i_max == i)\n",
    "    \n",
    "    X_test = np.delete(X_test, index_i, axis = 0 )\n",
    "#     print(\"After delete , X_test.shape  = \")\n",
    "#     print(X_test.shape)\n",
    "\n",
    "    \n",
    "    y_true_i = y_test[index_i]\n",
    "    \n",
    "    y_test = np.delete(y_test, index_i)\n",
    "    print(\"y_test.shape   = \")\n",
    "    print(y_test.shape)\n",
    "    \n",
    "\n",
    "#     print(index_i)\n",
    "    \n",
    "    y_true.append(y_true_i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=uint8)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25,)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1013\n",
      "\n",
      " Acc: 96.347 \n",
      "1\n",
      "1192\n",
      "\n",
      " Acc: 95.050 \n",
      "2\n",
      "1087\n",
      "\n",
      " Acc: 93.560 \n",
      "3\n",
      "1009\n",
      "\n",
      " Acc: 98.018 \n",
      "4\n",
      "986\n",
      "\n",
      " Acc: 97.972 \n",
      "5\n",
      "906\n",
      "\n",
      " Acc: 95.364 \n",
      "6\n",
      "915\n",
      "\n",
      " Acc: 99.454 \n",
      "7\n",
      "985\n",
      "\n",
      " Acc: 98.477 \n",
      "8\n",
      "927\n",
      "\n",
      " Acc: 99.353 \n",
      "9\n",
      "955\n",
      "\n",
      " Acc: 99.686 \n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,r2_score\n",
    "accs = []\n",
    "for i in range(10):\n",
    "    print(i)\n",
    "    sizes = y_true[i].size\n",
    "    \n",
    "    print(sizes)\n",
    "    y_pred = np.full(sizes, i)\n",
    "#     print(\"Predict --------------------->\")\n",
    "#     print(y_pred)\n",
    "    \n",
    "#     print(\"True------------------------->\")\n",
    "\n",
    "#     print(y_true[i])\n",
    "    \n",
    "    acc_i = accuracy_score(y_true[i],y_pred)*100\n",
    "    print(\"\\n Acc: %.3f \"% acc_i )\n",
    "    accs.append(acc_i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[96.34748272458046,\n",
       " 95.0503355704698,\n",
       " 93.56025758969642,\n",
       " 98.01783944499505,\n",
       " 97.97160243407707,\n",
       " 95.36423841059603,\n",
       " 99.4535519125683,\n",
       " 98.47715736040608,\n",
       " 99.35275080906149,\n",
       " 99.68586387434554]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97.32810801307963"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "av_acc = np.mean(accs)\n",
    "av_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the following things when training any type of deep neural network:\n",
    "\n",
    "1. the data used to calculate training accuracy is not identical to the data used to train your NN. This sounds weird, but possible in practice, especially in case of images, if you don't keep track of what is happening. For example, you train on random patches of images and calculate training accuracy on random patches of same images. It is easy to forget that though they are same images, the patches are randomly selected.\n",
    "2. More than the values of train and val accuracy, I would be concerned about what you said, \"i'm copy pasting a random epoch but all are roughly the same\". No, they can't be same. Accuracy at different epochs is mostly different, because network is learning so it is constantly changing its weights. If accuracy goes up then that means it is approaching the minima of the loss function.\n",
    "I think you should be more concerned about getting a low training accuracy instead of getting a lower training accuracy than the validation accuracy.\n",
    "3. Do all the sanity checks given here. Read the entire article if possible, it's very good.\n",
    "4. Make sure you are doing pre-processing in the right manner. For example, make sure that mean over entire training data is zero. For testing data, subtract the mean vector of the training data from each instance of testing data. Don't subtract the mean of testing data from itself. Since, you wouldn't know the mean of testing data at runtime.\n",
    "4. Check if your loss at the very first epoch makes sense. For example, in a 10-class classification problem, starting loss should be -ln(0.1) = 2.302 (given here).\n",
    "5. Again, from here, overfit a tiny subset of data and make sure you can achieve zero cost. Full details in the link.\n",
    "6. If nothing works, just train and test on the same data and see if you can get 90% + accuracy. Otherwise, examine your network more closely by looking at individual layer outputs (given in Keras FAQ) etc.\n",
    "\n",
    "\n",
    "<a href =\"https://github.com/keras-team/keras/issues/1761\">maybe solution</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
