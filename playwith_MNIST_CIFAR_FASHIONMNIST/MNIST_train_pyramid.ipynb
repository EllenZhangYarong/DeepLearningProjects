{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "from numpy import vstack\n",
    "from scipy.sparse import coo_matrix\n",
    "from sklearn.utils import shuffle\n",
    "from keras.utils import np_utils\n",
    "from sklearn.metrics import accuracy_score,r2_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.1\n"
     ]
    }
   ],
   "source": [
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 . Create training set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = './datasets/'\n",
    "def load_data(path, kind=''):\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path,\n",
    "                               '%s-labels-idx1-ubyte'\n",
    "                               % kind)\n",
    "    images_path = os.path.join(path,\n",
    "                               '%s-images-idx3-ubyte'\n",
    "                               % kind)\n",
    "    with open(labels_path, 'rb') as lbpath:\n",
    "        magic, n = struct.unpack('>II',\n",
    "                                 lbpath.read(8))\n",
    "        labels = np.fromfile(lbpath,\n",
    "                             dtype=np.uint8)\n",
    "\n",
    "    with open(images_path, 'rb') as imgpath:\n",
    "        magic, num, rows, cols = struct.unpack('>IIII',\n",
    "                                               imgpath.read(16))\n",
    "        images = np.fromfile(imgpath,\n",
    "                             dtype=np.uint8).reshape(len(labels), 784)\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Handwritten numbers training data\n",
    "\n",
    "X_train,y_train = load_data(path, kind='train')\n",
    "X_train = X_train / 255\n",
    "\n",
    "X_test, y_test = load_data(path, kind='t10k')\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  Testing set (mnist testing set)\n",
    "\n",
    "X_test  = X_test.reshape(len(X_test),1,28,28).astype('float')\n",
    "y_test  = np_utils.to_categorical(y_test,  num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "noise_random = np.random.rand(6000,784)\n",
    "noise_labels = np.full((6000), 10, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def flatten(x):\n",
    "    ''' Creates a generator object that loops through a nested list '''\n",
    "    # First see if the list is iterable\n",
    "    try:\n",
    "        it_is = iter(x)\n",
    "    # If it's not iterable return the list as is\n",
    "    except TypeError:\n",
    "        yield x\n",
    "    # If it is iterable, loop through the list recursively\n",
    "    else:\n",
    "        for i in it_is:\n",
    "            for j in flatten(i):\n",
    "                yield j\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_ds(X_train, y_train):\n",
    "    \n",
    "    X_train_split = []\n",
    "    y_train_split = []\n",
    "    indics = []\n",
    "\n",
    "    for i in range(15):    \n",
    "\n",
    "        if( i < 5):\n",
    "\n",
    "            index_i = np.where( y_train == i)\n",
    "            indics.append(index_i)\n",
    "\n",
    "\n",
    "        elif(i > 4 and i< 10):\n",
    "\n",
    "            index_i = np.where( y_train == i)\n",
    "            indics.append(index_i)        \n",
    "            indics.pop(0)\n",
    "\n",
    "        elif(i>9):\n",
    "\n",
    "            index_i = np.where( y_train == i-10)\n",
    "            indics.append(index_i)\n",
    "\n",
    "    #     flattened_list = np.array(indics)\n",
    "        indices_i = list(flatten(indics))\n",
    "\n",
    "        X_train_i, y_train_i = X_train[indices_i], y_train[indices_i]\n",
    "\n",
    "    #     noise_random = np.random.rand(60000,784)\n",
    "    #     noise_labels = np.full((60000), 10, dtype=int)\n",
    "\n",
    "    #     X_train_i = np.concatenate((X_train_i, noise_random),axis=0)\n",
    "    #     y_train_i = np.concatenate((y_train_i, noise_labels),axis=0)\n",
    "\n",
    "\n",
    "\n",
    "        X_sparse_train = coo_matrix(X_train_i)\n",
    "        X_train_i, X_sparse_train, y_train_i = shuffle(X_train_i, X_sparse_train, y_train_i, random_state=42)\n",
    "\n",
    "        X_train_i = X_train_i.reshape(len(X_train_i),1,28,28).astype('float')\n",
    "        y_train_i = np_utils.to_categorical(y_train_i, num_classes)\n",
    "    \n",
    "        X_train_split.append(X_train_i)\n",
    "        y_train_split.append(y_train_i)\n",
    "        \n",
    "    \n",
    "    return X_train_split, y_train_split\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_split, y_train_split = prepare_ds(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 1, 28, 28)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_split[14].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 . Build Model -- keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import Adadelta\n",
    "\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def models_factory():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', \n",
    "                     data_format='channels_first',\n",
    "                     input_shape=(1, 28, 28)))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(11, activation='softmax'))\n",
    "\n",
    "    model.compile(loss=categorical_crossentropy,\n",
    "                  optimizer=Adadelta(),\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 26, 26)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 30, 24, 32)        7520      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 15, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 15, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 13, 10, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 11, 8, 64)         36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 5, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                81984     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 11)                715       \n",
      "=================================================================\n",
      "Total params: 145,963\n",
      "Trainable params: 145,963\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = models_factory()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Your model needs be improved (change parameters)\n",
    "2. You may need to try a different machine learning algorithm (not all algorithms created equal)\n",
    "3. You need more data (subtle relationship difficult to find)\n",
    "4. You may need to try transforming your data (dependent upon algorithm used)\n",
    "5. There may be no relationship between your dependent and independent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_test_iter():\n",
    "    hs_history=[]\n",
    "\n",
    "    for i in range(15):\n",
    "\n",
    "        print(i)\n",
    "\n",
    "        X_train=X_train_split[i]\n",
    "        y_train = y_train_split[i]\n",
    "\n",
    "\n",
    "        history = model.fit(X_train, y_train, epochs=12, batch_size=32, validation_split = 0.3,verbose = 0)\n",
    "        hs_history.append(history)\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Train on 4738 samples, validate on 1185 samples\n",
      "Epoch 1/12\n",
      "4738/4738 [==============================] - 3s 563us/step - loss: 0.0403 - acc: 0.9930 - val_loss: 1.2107e-07 - val_acc: 1.0000\n",
      "Epoch 2/12\n",
      "4738/4738 [==============================] - 2s 445us/step - loss: 8.6903e-05 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 3/12\n",
      "4738/4738 [==============================] - 2s 408us/step - loss: 2.6398e-05 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 4/12\n",
      "4738/4738 [==============================] - 2s 414us/step - loss: 1.0489e-05 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 5/12\n",
      "4738/4738 [==============================] - 2s 412us/step - loss: 3.4516e-06 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 6/12\n",
      "4738/4738 [==============================] - 2s 407us/step - loss: 1.0451e-05 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 7/12\n",
      "4738/4738 [==============================] - 2s 420us/step - loss: 1.1614e-05 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 8/12\n",
      "4738/4738 [==============================] - 2s 421us/step - loss: 3.9261e-05 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 9/12\n",
      "4738/4738 [==============================] - 2s 409us/step - loss: 2.2437e-06 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 10/12\n",
      "4738/4738 [==============================] - 2s 412us/step - loss: 4.0931e-05 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 11/12\n",
      "4738/4738 [==============================] - 2s 417us/step - loss: 2.1159e-05 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 12/12\n",
      "4738/4738 [==============================] - 2s 416us/step - loss: 2.8426e-07 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "1\n",
      "Train on 10132 samples, validate on 2533 samples\n",
      "Epoch 1/12\n",
      "10132/10132 [==============================] - 4s 422us/step - loss: 0.9884 - acc: 0.9189 - val_loss: 0.0072 - val_acc: 0.9988\n",
      "Epoch 2/12\n",
      "10132/10132 [==============================] - 4s 410us/step - loss: 0.0089 - acc: 0.9979 - val_loss: 0.0077 - val_acc: 0.9980\n",
      "Epoch 3/12\n",
      "10132/10132 [==============================] - 4s 416us/step - loss: 0.0057 - acc: 0.9989 - val_loss: 0.0050 - val_acc: 0.9992\n",
      "Epoch 4/12\n",
      "10132/10132 [==============================] - 4s 407us/step - loss: 0.0036 - acc: 0.9988 - val_loss: 0.0026 - val_acc: 0.9996\n",
      "Epoch 5/12\n",
      "10132/10132 [==============================] - 4s 418us/step - loss: 0.0038 - acc: 0.9992 - val_loss: 0.0035 - val_acc: 0.9992\n",
      "Epoch 6/12\n",
      "10132/10132 [==============================] - 4s 408us/step - loss: 0.0034 - acc: 0.9992 - val_loss: 0.0068 - val_acc: 0.9984\n",
      "Epoch 7/12\n",
      "10132/10132 [==============================] - 4s 411us/step - loss: 0.0020 - acc: 0.9995 - val_loss: 0.0052 - val_acc: 0.9996\n",
      "Epoch 8/12\n",
      "10132/10132 [==============================] - 4s 410us/step - loss: 0.0028 - acc: 0.9992 - val_loss: 0.0101 - val_acc: 0.9980\n",
      "Epoch 9/12\n",
      "10132/10132 [==============================] - 4s 408us/step - loss: 0.0018 - acc: 0.9995 - val_loss: 0.0090 - val_acc: 0.9984\n",
      "Epoch 10/12\n",
      "10132/10132 [==============================] - 4s 410us/step - loss: 8.8234e-04 - acc: 0.9998 - val_loss: 0.0134 - val_acc: 0.9980\n",
      "Epoch 11/12\n",
      "10132/10132 [==============================] - 4s 408us/step - loss: 0.0021 - acc: 0.9992 - val_loss: 0.0071 - val_acc: 0.9988\n",
      "Epoch 12/12\n",
      "10132/10132 [==============================] - 4s 410us/step - loss: 7.2123e-04 - acc: 0.9998 - val_loss: 0.0077 - val_acc: 0.9988\n",
      "2\n",
      "Train on 14898 samples, validate on 3725 samples\n",
      "Epoch 1/12\n",
      "14898/14898 [==============================] - 6s 411us/step - loss: 0.1420 - acc: 0.9644 - val_loss: 0.0252 - val_acc: 0.9928\n",
      "Epoch 2/12\n",
      "14898/14898 [==============================] - 6s 407us/step - loss: 0.0402 - acc: 0.9884 - val_loss: 0.0185 - val_acc: 0.9933\n",
      "Epoch 3/12\n",
      "14898/14898 [==============================] - 6s 412us/step - loss: 0.0272 - acc: 0.9925 - val_loss: 0.0168 - val_acc: 0.9957\n",
      "Epoch 4/12\n",
      "14898/14898 [==============================] - 6s 410us/step - loss: 0.0214 - acc: 0.9947 - val_loss: 0.0110 - val_acc: 0.9962\n",
      "Epoch 5/12\n",
      "14898/14898 [==============================] - 6s 408us/step - loss: 0.0156 - acc: 0.9954 - val_loss: 0.0086 - val_acc: 0.9970\n",
      "Epoch 6/12\n",
      "14898/14898 [==============================] - 6s 417us/step - loss: 0.0148 - acc: 0.9965 - val_loss: 0.0071 - val_acc: 0.9984\n",
      "Epoch 7/12\n",
      "14898/14898 [==============================] - 6s 417us/step - loss: 0.0116 - acc: 0.9968 - val_loss: 0.0077 - val_acc: 0.9981\n",
      "Epoch 8/12\n",
      "14898/14898 [==============================] - 6s 419us/step - loss: 0.0095 - acc: 0.9970 - val_loss: 0.0077 - val_acc: 0.9984\n",
      "Epoch 9/12\n",
      "14898/14898 [==============================] - 6s 409us/step - loss: 0.0074 - acc: 0.9983 - val_loss: 0.0085 - val_acc: 0.9984\n",
      "Epoch 10/12\n",
      "14898/14898 [==============================] - 6s 414us/step - loss: 0.0090 - acc: 0.9974 - val_loss: 0.0068 - val_acc: 0.9981\n",
      "Epoch 11/12\n",
      "14898/14898 [==============================] - 6s 414us/step - loss: 0.0076 - acc: 0.9981 - val_loss: 0.0061 - val_acc: 0.9984\n",
      "Epoch 12/12\n",
      "14898/14898 [==============================] - 6s 409us/step - loss: 0.0067 - acc: 0.9983 - val_loss: 0.0054 - val_acc: 0.9987\n",
      "3\n",
      "Train on 19803 samples, validate on 4951 samples\n",
      "Epoch 1/12\n",
      "19803/19803 [==============================] - 8s 423us/step - loss: 0.1134 - acc: 0.9748 - val_loss: 0.0172 - val_acc: 0.9950\n",
      "Epoch 2/12\n",
      "19803/19803 [==============================] - 8s 410us/step - loss: 0.0281 - acc: 0.9927 - val_loss: 0.0091 - val_acc: 0.9970\n",
      "Epoch 3/12\n",
      "19803/19803 [==============================] - 8s 410us/step - loss: 0.0198 - acc: 0.9942 - val_loss: 0.0148 - val_acc: 0.9970\n",
      "Epoch 4/12\n",
      "19803/19803 [==============================] - 8s 418us/step - loss: 0.0211 - acc: 0.9943 - val_loss: 0.0071 - val_acc: 0.9978\n",
      "Epoch 5/12\n",
      "19803/19803 [==============================] - 8s 414us/step - loss: 0.0176 - acc: 0.9953 - val_loss: 0.0103 - val_acc: 0.9980\n",
      "Epoch 6/12\n",
      "19803/19803 [==============================] - 8s 421us/step - loss: 0.0162 - acc: 0.9956 - val_loss: 0.0080 - val_acc: 0.9974\n",
      "Epoch 7/12\n",
      "19803/19803 [==============================] - 8s 421us/step - loss: 0.0125 - acc: 0.9968 - val_loss: 0.0097 - val_acc: 0.9982\n",
      "Epoch 8/12\n",
      "19803/19803 [==============================] - 8s 415us/step - loss: 0.0121 - acc: 0.9966 - val_loss: 0.0106 - val_acc: 0.9974\n",
      "Epoch 9/12\n",
      "19803/19803 [==============================] - 8s 415us/step - loss: 0.0111 - acc: 0.9970 - val_loss: 0.0084 - val_acc: 0.9980\n",
      "Epoch 10/12\n",
      "19803/19803 [==============================] - 8s 410us/step - loss: 0.0112 - acc: 0.9972 - val_loss: 0.0095 - val_acc: 0.9976\n",
      "Epoch 11/12\n",
      "19803/19803 [==============================] - 8s 418us/step - loss: 0.0078 - acc: 0.9976 - val_loss: 0.0062 - val_acc: 0.9984\n",
      "Epoch 12/12\n",
      "19803/19803 [==============================] - 8s 416us/step - loss: 0.0084 - acc: 0.9974 - val_loss: 0.0070 - val_acc: 0.9982\n",
      "4\n",
      "Train on 24476 samples, validate on 6120 samples\n",
      "Epoch 1/12\n",
      "24476/24476 [==============================] - 10s 419us/step - loss: 0.0836 - acc: 0.9755 - val_loss: 0.0181 - val_acc: 0.9946\n",
      "Epoch 2/12\n",
      "24476/24476 [==============================] - 10s 412us/step - loss: 0.0259 - acc: 0.9936 - val_loss: 0.0183 - val_acc: 0.9954\n",
      "Epoch 3/12\n",
      "24476/24476 [==============================] - 10s 412us/step - loss: 0.0190 - acc: 0.9956 - val_loss: 0.0123 - val_acc: 0.9975\n",
      "Epoch 4/12\n",
      "24476/24476 [==============================] - 10s 411us/step - loss: 0.0144 - acc: 0.9962 - val_loss: 0.0126 - val_acc: 0.9969\n",
      "Epoch 5/12\n",
      "24476/24476 [==============================] - 10s 421us/step - loss: 0.0126 - acc: 0.9967 - val_loss: 0.0176 - val_acc: 0.9971\n",
      "Epoch 6/12\n",
      "24476/24476 [==============================] - 10s 420us/step - loss: 0.0125 - acc: 0.9970 - val_loss: 0.0122 - val_acc: 0.9974\n",
      "Epoch 7/12\n",
      "24476/24476 [==============================] - 10s 420us/step - loss: 0.0118 - acc: 0.9969 - val_loss: 0.0179 - val_acc: 0.9964\n",
      "Epoch 8/12\n",
      "24476/24476 [==============================] - 10s 419us/step - loss: 0.0122 - acc: 0.9973 - val_loss: 0.0185 - val_acc: 0.9959\n",
      "Epoch 9/12\n",
      "24476/24476 [==============================] - 10s 421us/step - loss: 0.0103 - acc: 0.9976 - val_loss: 0.0148 - val_acc: 0.9969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/12\n",
      "24476/24476 [==============================] - 10s 422us/step - loss: 0.0093 - acc: 0.9973 - val_loss: 0.0106 - val_acc: 0.9972\n",
      "Epoch 11/12\n",
      "24476/24476 [==============================] - 10s 423us/step - loss: 0.0091 - acc: 0.9976 - val_loss: 0.0152 - val_acc: 0.9972\n",
      "Epoch 12/12\n",
      "24476/24476 [==============================] - 10s 427us/step - loss: 0.0087 - acc: 0.9978 - val_loss: 0.0089 - val_acc: 0.9982\n",
      "5\n",
      "Train on 24075 samples, validate on 6019 samples\n",
      "Epoch 1/12\n",
      "24075/24075 [==============================] - 10s 413us/step - loss: 0.1172 - acc: 0.9683 - val_loss: 0.0278 - val_acc: 0.9930\n",
      "Epoch 2/12\n",
      "24075/24075 [==============================] - 10s 417us/step - loss: 0.0279 - acc: 0.9919 - val_loss: 0.0236 - val_acc: 0.9934\n",
      "Epoch 3/12\n",
      "24075/24075 [==============================] - 10s 412us/step - loss: 0.0234 - acc: 0.9937 - val_loss: 0.0221 - val_acc: 0.9955\n",
      "Epoch 4/12\n",
      "24075/24075 [==============================] - 10s 426us/step - loss: 0.0178 - acc: 0.9952 - val_loss: 0.0164 - val_acc: 0.9962\n",
      "Epoch 5/12\n",
      "24075/24075 [==============================] - 10s 418us/step - loss: 0.0163 - acc: 0.9956 - val_loss: 0.0194 - val_acc: 0.9960\n",
      "Epoch 6/12\n",
      "24075/24075 [==============================] - 10s 426us/step - loss: 0.0158 - acc: 0.9958 - val_loss: 0.0211 - val_acc: 0.9955\n",
      "Epoch 7/12\n",
      "24075/24075 [==============================] - 10s 421us/step - loss: 0.0138 - acc: 0.9963 - val_loss: 0.0179 - val_acc: 0.9968\n",
      "Epoch 8/12\n",
      "24075/24075 [==============================] - 10s 426us/step - loss: 0.0135 - acc: 0.9964 - val_loss: 0.0285 - val_acc: 0.9935\n",
      "Epoch 9/12\n",
      "24075/24075 [==============================] - 10s 422us/step - loss: 0.0116 - acc: 0.9970 - val_loss: 0.0269 - val_acc: 0.9963\n",
      "Epoch 10/12\n",
      "24075/24075 [==============================] - 10s 424us/step - loss: 0.0100 - acc: 0.9972 - val_loss: 0.0227 - val_acc: 0.9962\n",
      "Epoch 11/12\n",
      "24075/24075 [==============================] - 10s 422us/step - loss: 0.0123 - acc: 0.9971 - val_loss: 0.0328 - val_acc: 0.9948\n",
      "Epoch 12/12\n",
      "24075/24075 [==============================] - 10s 422us/step - loss: 0.0107 - acc: 0.9971 - val_loss: 0.0236 - val_acc: 0.9955\n",
      "6\n",
      "Train on 23416 samples, validate on 5854 samples\n",
      "Epoch 1/12\n",
      "23416/23416 [==============================] - 10s 426us/step - loss: 0.1455 - acc: 0.9537 - val_loss: 0.0159 - val_acc: 0.9954\n",
      "Epoch 2/12\n",
      "23416/23416 [==============================] - 10s 417us/step - loss: 0.0392 - acc: 0.9898 - val_loss: 0.0096 - val_acc: 0.9976\n",
      "Epoch 3/12\n",
      "23416/23416 [==============================] - 10s 419us/step - loss: 0.0295 - acc: 0.9922 - val_loss: 0.0119 - val_acc: 0.9962\n",
      "Epoch 4/12\n",
      "23416/23416 [==============================] - 10s 413us/step - loss: 0.0265 - acc: 0.9936 - val_loss: 0.0118 - val_acc: 0.9968\n",
      "Epoch 5/12\n",
      "23416/23416 [==============================] - 10s 415us/step - loss: 0.0218 - acc: 0.9953 - val_loss: 0.0108 - val_acc: 0.9973\n",
      "Epoch 6/12\n",
      "23416/23416 [==============================] - 10s 410us/step - loss: 0.0195 - acc: 0.9951 - val_loss: 0.0092 - val_acc: 0.9974\n",
      "Epoch 7/12\n",
      "23416/23416 [==============================] - 10s 411us/step - loss: 0.0211 - acc: 0.9950 - val_loss: 0.0127 - val_acc: 0.9971\n",
      "Epoch 8/12\n",
      "23416/23416 [==============================] - 10s 417us/step - loss: 0.0148 - acc: 0.9960 - val_loss: 0.0098 - val_acc: 0.9981\n",
      "Epoch 9/12\n",
      "23416/23416 [==============================] - 10s 421us/step - loss: 0.0158 - acc: 0.9959 - val_loss: 0.0152 - val_acc: 0.9962\n",
      "Epoch 10/12\n",
      "23416/23416 [==============================] - 10s 419us/step - loss: 0.0154 - acc: 0.9963 - val_loss: 0.0111 - val_acc: 0.9973\n",
      "Epoch 11/12\n",
      "23416/23416 [==============================] - 10s 417us/step - loss: 0.0161 - acc: 0.9969 - val_loss: 0.0196 - val_acc: 0.9954\n",
      "Epoch 12/12\n",
      "23416/23416 [==============================] - 10s 417us/step - loss: 0.0130 - acc: 0.9970 - val_loss: 0.0137 - val_acc: 0.9968\n",
      "7\n",
      "Train on 23661 samples, validate on 5916 samples\n",
      "Epoch 1/12\n",
      "23661/23661 [==============================] - 10s 423us/step - loss: 0.1886 - acc: 0.9410 - val_loss: 0.0176 - val_acc: 0.9961\n",
      "Epoch 2/12\n",
      "23661/23661 [==============================] - 10s 410us/step - loss: 0.0365 - acc: 0.9908 - val_loss: 0.0195 - val_acc: 0.9959\n",
      "Epoch 3/12\n",
      "23661/23661 [==============================] - 10s 409us/step - loss: 0.0273 - acc: 0.9929 - val_loss: 0.0158 - val_acc: 0.9968\n",
      "Epoch 4/12\n",
      "23661/23661 [==============================] - 10s 410us/step - loss: 0.0247 - acc: 0.9936 - val_loss: 0.0194 - val_acc: 0.9966\n",
      "Epoch 5/12\n",
      "23661/23661 [==============================] - 10s 413us/step - loss: 0.0203 - acc: 0.9950 - val_loss: 0.0208 - val_acc: 0.9958\n",
      "Epoch 6/12\n",
      "23661/23661 [==============================] - 10s 420us/step - loss: 0.0211 - acc: 0.9951 - val_loss: 0.0184 - val_acc: 0.9971\n",
      "Epoch 7/12\n",
      "23661/23661 [==============================] - 10s 412us/step - loss: 0.0179 - acc: 0.9958 - val_loss: 0.0264 - val_acc: 0.9939\n",
      "Epoch 8/12\n",
      "23661/23661 [==============================] - 10s 411us/step - loss: 0.0164 - acc: 0.9959 - val_loss: 0.0195 - val_acc: 0.9966\n",
      "Epoch 9/12\n",
      "23661/23661 [==============================] - 10s 413us/step - loss: 0.0140 - acc: 0.9961 - val_loss: 0.0220 - val_acc: 0.9954\n",
      "Epoch 10/12\n",
      "23661/23661 [==============================] - 10s 418us/step - loss: 0.0146 - acc: 0.9967 - val_loss: 0.0282 - val_acc: 0.9961\n",
      "Epoch 11/12\n",
      "23661/23661 [==============================] - 10s 417us/step - loss: 0.0158 - acc: 0.9961 - val_loss: 0.0240 - val_acc: 0.9954\n",
      "Epoch 12/12\n",
      "23661/23661 [==============================] - 10s 416us/step - loss: 0.0124 - acc: 0.9971 - val_loss: 0.0218 - val_acc: 0.9961\n",
      "8\n",
      "Train on 23437 samples, validate on 5860 samples\n",
      "Epoch 1/12\n",
      "23437/23437 [==============================] - 10s 416us/step - loss: 0.2627 - acc: 0.9229 - val_loss: 0.0464 - val_acc: 0.9903\n",
      "Epoch 2/12\n",
      "23437/23437 [==============================] - 10s 420us/step - loss: 0.0675 - acc: 0.9800 - val_loss: 0.0237 - val_acc: 0.9942\n",
      "Epoch 3/12\n",
      "23437/23437 [==============================] - 10s 418us/step - loss: 0.0466 - acc: 0.9880 - val_loss: 0.0233 - val_acc: 0.9945\n",
      "Epoch 4/12\n",
      "23437/23437 [==============================] - 10s 423us/step - loss: 0.0413 - acc: 0.9895 - val_loss: 0.0287 - val_acc: 0.9933\n",
      "Epoch 5/12\n",
      "23437/23437 [==============================] - 10s 415us/step - loss: 0.0335 - acc: 0.9910 - val_loss: 0.0222 - val_acc: 0.9951\n",
      "Epoch 6/12\n",
      "23437/23437 [==============================] - 10s 418us/step - loss: 0.0311 - acc: 0.9922 - val_loss: 0.0252 - val_acc: 0.9951\n",
      "Epoch 7/12\n",
      "23437/23437 [==============================] - 10s 418us/step - loss: 0.0260 - acc: 0.9946 - val_loss: 0.0231 - val_acc: 0.9949\n",
      "Epoch 8/12\n",
      "23437/23437 [==============================] - 10s 416us/step - loss: 0.0227 - acc: 0.9948 - val_loss: 0.0315 - val_acc: 0.9951\n",
      "Epoch 9/12\n",
      "23437/23437 [==============================] - 10s 424us/step - loss: 0.0203 - acc: 0.9957 - val_loss: 0.0340 - val_acc: 0.9937\n",
      "Epoch 10/12\n",
      "23437/23437 [==============================] - 10s 418us/step - loss: 0.0208 - acc: 0.9950 - val_loss: 0.0580 - val_acc: 0.9908\n",
      "Epoch 11/12\n",
      "23437/23437 [==============================] - 10s 414us/step - loss: 0.0186 - acc: 0.9959 - val_loss: 0.0245 - val_acc: 0.9954\n",
      "Epoch 12/12\n",
      "23437/23437 [==============================] - 10s 416us/step - loss: 0.0214 - acc: 0.9953 - val_loss: 0.0344 - val_acc: 0.9949\n",
      "9\n",
      "Train on 23523 samples, validate on 5881 samples\n",
      "Epoch 1/12\n",
      "23523/23523 [==============================] - 10s 421us/step - loss: 0.4632 - acc: 0.8721 - val_loss: 0.0495 - val_acc: 0.9923\n",
      "Epoch 2/12\n",
      "23523/23523 [==============================] - 10s 420us/step - loss: 0.1703 - acc: 0.9394 - val_loss: 0.0159 - val_acc: 0.9951\n",
      "Epoch 3/12\n",
      "23523/23523 [==============================] - 10s 415us/step - loss: 0.0865 - acc: 0.9716 - val_loss: 0.0297 - val_acc: 0.9908\n",
      "Epoch 4/12\n",
      "23523/23523 [==============================] - 10s 412us/step - loss: 0.0601 - acc: 0.9817 - val_loss: 0.0215 - val_acc: 0.9949\n",
      "Epoch 5/12\n",
      "23523/23523 [==============================] - 10s 414us/step - loss: 0.0499 - acc: 0.9864 - val_loss: 0.0320 - val_acc: 0.9937\n",
      "Epoch 6/12\n",
      "23523/23523 [==============================] - 10s 412us/step - loss: 0.0456 - acc: 0.9898 - val_loss: 0.0230 - val_acc: 0.9940\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/12\n",
      "23523/23523 [==============================] - 10s 426us/step - loss: 0.0427 - acc: 0.9898 - val_loss: 0.0221 - val_acc: 0.9954\n",
      "Epoch 8/12\n",
      "23523/23523 [==============================] - 10s 408us/step - loss: 0.0423 - acc: 0.9897 - val_loss: 0.0174 - val_acc: 0.9956\n",
      "Epoch 9/12\n",
      "23523/23523 [==============================] - 10s 418us/step - loss: 0.0344 - acc: 0.9910 - val_loss: 0.0281 - val_acc: 0.9947\n",
      "Epoch 10/12\n",
      "23523/23523 [==============================] - 10s 426us/step - loss: 0.0357 - acc: 0.9911 - val_loss: 0.0238 - val_acc: 0.9957\n",
      "Epoch 11/12\n",
      "23523/23523 [==============================] - 10s 426us/step - loss: 0.0348 - acc: 0.9919 - val_loss: 0.0416 - val_acc: 0.9932\n",
      "Epoch 12/12\n",
      "23523/23523 [==============================] - 10s 422us/step - loss: 0.0357 - acc: 0.9911 - val_loss: 0.0203 - val_acc: 0.9954\n",
      "10\n",
      "Train on 28261 samples, validate on 7066 samples\n",
      "Epoch 1/12\n",
      "28261/28261 [==============================] - 12s 415us/step - loss: 0.2779 - acc: 0.9303 - val_loss: 0.0452 - val_acc: 0.9917\n",
      "Epoch 2/12\n",
      "28261/28261 [==============================] - 12s 416us/step - loss: 0.0668 - acc: 0.9822 - val_loss: 0.0259 - val_acc: 0.9943\n",
      "Epoch 3/12\n",
      "28261/28261 [==============================] - 12s 415us/step - loss: 0.0504 - acc: 0.9878 - val_loss: 0.0340 - val_acc: 0.9933\n",
      "Epoch 4/12\n",
      "28261/28261 [==============================] - 12s 424us/step - loss: 0.0459 - acc: 0.9889 - val_loss: 0.0279 - val_acc: 0.9943\n",
      "Epoch 5/12\n",
      "28261/28261 [==============================] - 12s 429us/step - loss: 0.0407 - acc: 0.9900 - val_loss: 0.0212 - val_acc: 0.9958\n",
      "Epoch 6/12\n",
      "28261/28261 [==============================] - 12s 418us/step - loss: 0.0377 - acc: 0.9901 - val_loss: 0.0293 - val_acc: 0.9946\n",
      "Epoch 7/12\n",
      "28261/28261 [==============================] - 12s 415us/step - loss: 0.0375 - acc: 0.9909 - val_loss: 0.0315 - val_acc: 0.9950\n",
      "Epoch 8/12\n",
      "28261/28261 [==============================] - 12s 414us/step - loss: 0.0397 - acc: 0.9902 - val_loss: 0.0225 - val_acc: 0.9941\n",
      "Epoch 9/12\n",
      "28261/28261 [==============================] - 12s 416us/step - loss: 0.0367 - acc: 0.9902 - val_loss: 0.0421 - val_acc: 0.9932\n",
      "Epoch 10/12\n",
      "28261/28261 [==============================] - 12s 413us/step - loss: 0.0355 - acc: 0.9910 - val_loss: 0.0288 - val_acc: 0.9943\n",
      "Epoch 11/12\n",
      "28261/28261 [==============================] - 12s 411us/step - loss: 0.0342 - acc: 0.9919 - val_loss: 0.0462 - val_acc: 0.9928\n",
      "Epoch 12/12\n",
      "28261/28261 [==============================] - 12s 414us/step - loss: 0.0342 - acc: 0.9923 - val_loss: 0.0294 - val_acc: 0.9945\n",
      "11\n",
      "Train on 33655 samples, validate on 8414 samples\n",
      "Epoch 1/12\n",
      "33655/33655 [==============================] - 14s 418us/step - loss: 0.2708 - acc: 0.9359 - val_loss: 0.0341 - val_acc: 0.9941\n",
      "Epoch 2/12\n",
      "33655/33655 [==============================] - 14s 413us/step - loss: 0.0749 - acc: 0.9798 - val_loss: 0.0447 - val_acc: 0.9926\n",
      "Epoch 3/12\n",
      "33655/33655 [==============================] - 14s 417us/step - loss: 0.0599 - acc: 0.9862 - val_loss: 0.0482 - val_acc: 0.9903\n",
      "Epoch 4/12\n",
      "33655/33655 [==============================] - 14s 414us/step - loss: 0.0550 - acc: 0.9874 - val_loss: 0.0351 - val_acc: 0.9936\n",
      "Epoch 5/12\n",
      "33655/33655 [==============================] - 14s 421us/step - loss: 0.0480 - acc: 0.9886 - val_loss: 0.0425 - val_acc: 0.9922\n",
      "Epoch 6/12\n",
      "33655/33655 [==============================] - 14s 422us/step - loss: 0.0447 - acc: 0.9892 - val_loss: 0.0451 - val_acc: 0.9926\n",
      "Epoch 7/12\n",
      "33655/33655 [==============================] - 14s 416us/step - loss: 0.0456 - acc: 0.9900 - val_loss: 0.0414 - val_acc: 0.9948\n",
      "Epoch 8/12\n",
      "33655/33655 [==============================] - 14s 412us/step - loss: 0.0454 - acc: 0.9887 - val_loss: 0.0311 - val_acc: 0.9935\n",
      "Epoch 9/12\n",
      "33655/33655 [==============================] - 14s 412us/step - loss: 0.0427 - acc: 0.9906 - val_loss: 0.0438 - val_acc: 0.9916\n",
      "Epoch 10/12\n",
      "33655/33655 [==============================] - 14s 412us/step - loss: 0.0443 - acc: 0.9895 - val_loss: 0.0404 - val_acc: 0.9939\n",
      "Epoch 11/12\n",
      "33655/33655 [==============================] - 14s 418us/step - loss: 0.0427 - acc: 0.9906 - val_loss: 0.0407 - val_acc: 0.9923\n",
      "Epoch 12/12\n",
      "33655/33655 [==============================] - 14s 417us/step - loss: 0.0408 - acc: 0.9908 - val_loss: 0.0476 - val_acc: 0.9931\n",
      "12\n",
      "Train on 38421 samples, validate on 9606 samples\n",
      "Epoch 1/12\n",
      "38421/38421 [==============================] - 16s 422us/step - loss: 0.2479 - acc: 0.9407 - val_loss: 0.0474 - val_acc: 0.9901\n",
      "Epoch 2/12\n",
      "38421/38421 [==============================] - 16s 415us/step - loss: 0.0831 - acc: 0.9793 - val_loss: 0.0555 - val_acc: 0.9904\n",
      "Epoch 3/12\n",
      "38421/38421 [==============================] - 16s 416us/step - loss: 0.0737 - acc: 0.9823 - val_loss: 0.0690 - val_acc: 0.9871\n",
      "Epoch 4/12\n",
      "38421/38421 [==============================] - 16s 415us/step - loss: 0.0706 - acc: 0.9830 - val_loss: 0.0502 - val_acc: 0.9900\n",
      "Epoch 5/12\n",
      "38421/38421 [==============================] - 16s 416us/step - loss: 0.0646 - acc: 0.9841 - val_loss: 0.0589 - val_acc: 0.9902\n",
      "Epoch 6/12\n",
      "38421/38421 [==============================] - 16s 419us/step - loss: 0.0693 - acc: 0.9841 - val_loss: 0.0479 - val_acc: 0.9917\n",
      "Epoch 7/12\n",
      "38421/38421 [==============================] - 16s 415us/step - loss: 0.0659 - acc: 0.9850 - val_loss: 0.0505 - val_acc: 0.9902\n",
      "Epoch 8/12\n",
      "38421/38421 [==============================] - 16s 415us/step - loss: 0.0632 - acc: 0.9858 - val_loss: 0.0538 - val_acc: 0.9907\n",
      "Epoch 9/12\n",
      "38421/38421 [==============================] - 16s 414us/step - loss: 0.0660 - acc: 0.9837 - val_loss: 0.0595 - val_acc: 0.9913\n",
      "Epoch 10/12\n",
      "38421/38421 [==============================] - 16s 415us/step - loss: 0.0605 - acc: 0.9858 - val_loss: 0.0480 - val_acc: 0.9891\n",
      "Epoch 11/12\n",
      "38421/38421 [==============================] - 16s 421us/step - loss: 0.0599 - acc: 0.9862 - val_loss: 0.0450 - val_acc: 0.9918\n",
      "Epoch 12/12\n",
      "38421/38421 [==============================] - 16s 425us/step - loss: 0.0629 - acc: 0.9856 - val_loss: 0.0497 - val_acc: 0.9919\n",
      "13\n",
      "Train on 43326 samples, validate on 10832 samples\n",
      "Epoch 1/12\n",
      "43326/43326 [==============================] - 18s 423us/step - loss: 0.2178 - acc: 0.9468 - val_loss: 0.0684 - val_acc: 0.9892\n",
      "Epoch 2/12\n",
      "43326/43326 [==============================] - 18s 424us/step - loss: 0.0924 - acc: 0.9785 - val_loss: 0.0593 - val_acc: 0.9887\n",
      "Epoch 3/12\n",
      "43326/43326 [==============================] - 18s 424us/step - loss: 0.0861 - acc: 0.9797 - val_loss: 0.0748 - val_acc: 0.9870\n",
      "Epoch 4/12\n",
      "43326/43326 [==============================] - 18s 415us/step - loss: 0.0832 - acc: 0.9813 - val_loss: 0.0711 - val_acc: 0.9850\n",
      "Epoch 5/12\n",
      "43326/43326 [==============================] - 18s 419us/step - loss: 0.0763 - acc: 0.9820 - val_loss: 0.0574 - val_acc: 0.9906\n",
      "Epoch 6/12\n",
      "43326/43326 [==============================] - 18s 407us/step - loss: 0.0728 - acc: 0.9831 - val_loss: 0.0637 - val_acc: 0.9903\n",
      "Epoch 7/12\n",
      "43326/43326 [==============================] - 18s 421us/step - loss: 0.0734 - acc: 0.9826 - val_loss: 0.0522 - val_acc: 0.9890\n",
      "Epoch 8/12\n",
      "43326/43326 [==============================] - 18s 421us/step - loss: 0.0737 - acc: 0.9833 - val_loss: 0.0740 - val_acc: 0.9811\n",
      "Epoch 9/12\n",
      "43326/43326 [==============================] - 18s 418us/step - loss: 0.0762 - acc: 0.9833 - val_loss: 0.0537 - val_acc: 0.9902\n",
      "Epoch 10/12\n",
      "43326/43326 [==============================] - 18s 414us/step - loss: 0.0799 - acc: 0.9832 - val_loss: 0.0577 - val_acc: 0.9896\n",
      "Epoch 11/12\n",
      "43326/43326 [==============================] - 18s 419us/step - loss: 0.0731 - acc: 0.9840 - val_loss: 0.0704 - val_acc: 0.9874\n",
      "Epoch 12/12\n",
      "43326/43326 [==============================] - 18s 412us/step - loss: 0.0733 - acc: 0.9840 - val_loss: 0.0622 - val_acc: 0.9882\n",
      "14\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/12\n",
      "48000/48000 [==============================] - 20s 419us/step - loss: 0.2130 - acc: 0.9519 - val_loss: 0.0807 - val_acc: 0.9861\n",
      "Epoch 2/12\n",
      "48000/48000 [==============================] - 20s 415us/step - loss: 0.1070 - acc: 0.9762 - val_loss: 0.0637 - val_acc: 0.9858\n",
      "Epoch 3/12\n",
      "48000/48000 [==============================] - 20s 422us/step - loss: 0.1096 - acc: 0.9765 - val_loss: 0.0673 - val_acc: 0.9887\n",
      "Epoch 4/12\n",
      "48000/48000 [==============================] - 20s 419us/step - loss: 0.0995 - acc: 0.9793 - val_loss: 0.0697 - val_acc: 0.9859\n",
      "Epoch 5/12\n",
      "48000/48000 [==============================] - 20s 417us/step - loss: 0.0960 - acc: 0.9795 - val_loss: 0.0685 - val_acc: 0.9849\n",
      "Epoch 6/12\n",
      "48000/48000 [==============================] - 20s 413us/step - loss: 0.0934 - acc: 0.9809 - val_loss: 0.0631 - val_acc: 0.9888\n",
      "Epoch 7/12\n",
      "48000/48000 [==============================] - 20s 421us/step - loss: 0.1010 - acc: 0.9802 - val_loss: 0.0874 - val_acc: 0.9859\n",
      "Epoch 8/12\n",
      "48000/48000 [==============================] - 20s 420us/step - loss: 0.0921 - acc: 0.9811 - val_loss: 0.0718 - val_acc: 0.9867\n",
      "Epoch 9/12\n",
      "48000/48000 [==============================] - 20s 421us/step - loss: 0.0992 - acc: 0.9800 - val_loss: 0.0791 - val_acc: 0.9859\n",
      "Epoch 10/12\n",
      "48000/48000 [==============================] - 20s 416us/step - loss: 0.0891 - acc: 0.9813 - val_loss: 0.0644 - val_acc: 0.9894\n",
      "Epoch 11/12\n",
      "48000/48000 [==============================] - 20s 415us/step - loss: 0.0948 - acc: 0.9803 - val_loss: 0.0741 - val_acc: 0.9868\n",
      "Epoch 12/12\n",
      "48000/48000 [==============================] - 20s 414us/step - loss: 0.0895 - acc: 0.9818 - val_loss: 0.0839 - val_acc: 0.9856\n",
      "10000/10000 [==============================] - 1s 124us/step\n",
      "1 : Mnist pure test sets -> Loss: 7.65%\n",
      "1 : Mnist pure test sets -> Accuracy: 98.57%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model.fit(X_train, y_train, epochs=12, batch_size=32)\n",
    "hs_train_scores = []\n",
    "hs_test_scores = []\n",
    "\n",
    "\n",
    "for i in range(1):\n",
    "    \n",
    "    model_trained = train_test_iter()\n",
    "    \n",
    "    test_score = model_trained.evaluate(X_test, y_test, verbose=1)\n",
    "    hs_test_scores.append(test_score)\n",
    "    print(\"%d : Mnist pure test sets -> Loss: %.2f%%\" % (i+1, test_score[0]*100))\n",
    "    print(\"%d : Mnist pure test sets -> Accuracy: %.2f%%\" % (i+1, test_score[1]*100))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.fit(X_train, y_train, epochs=12, batch_size=32)\n",
    "hs_train_scores = []\n",
    "hs_test_scores = []\n",
    "hs_history=[]\n",
    "for i in range(30):\n",
    "        \n",
    "    history = model.fit(X_train, y_train, epochs=12, batch_size=32,validation_split=0.3,verbose=0)\n",
    "    hs_history.append(history)\n",
    "    \n",
    "    \n",
    "    print(\"------------------------%d : training --------------------------------------\" % (i+1))\n",
    "    train_score = model.evaluate(X_train, y_train, verbose=1)\n",
    "    hs_train_scores.append(train_score)\n",
    "    print(\"%d : Mnist Training -> Loss: %.2f%%\" % (i+1, train_score[0]*100))\n",
    "    print(\"%d : Mnist Training -> Accuracy: %.2f%%\" % (i+1, train_score[1]*100))\n",
    "    print();\n",
    "    print(\"------------------------%d : test ------------------------------------------\" % (i+1))\n",
    "    test_score = model.evaluate(X_test, y_test, verbose=1)\n",
    "    hs_test_scores.append(test_score)\n",
    "    print(\"%d : Mnist pure test sets -> Loss: %.2f%%\" % (i+1, test_score[0]*100))\n",
    "    print(\"%d : Mnist pure test sets -> Accuracy: %.2f%%\" % (i+1, test_score[1]*100))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(\"./models/MNIST_with_mixed.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the following things when training any type of deep neural network:\n",
    "\n",
    "1. the data used to calculate training accuracy is not identical to the data used to train your NN. This sounds weird, but possible in practice, especially in case of images, if you don't keep track of what is happening. For example, you train on random patches of images and calculate training accuracy on random patches of same images. It is easy to forget that though they are same images, the patches are randomly selected.\n",
    "2. More than the values of train and val accuracy, I would be concerned about what you said, \"i'm copy pasting a random epoch but all are roughly the same\". No, they can't be same. Accuracy at different epochs is mostly different, because network is learning so it is constantly changing its weights. If accuracy goes up then that means it is approaching the minima of the loss function.\n",
    "I think you should be more concerned about getting a low training accuracy instead of getting a lower training accuracy than the validation accuracy.\n",
    "3. Do all the sanity checks given here. Read the entire article if possible, it's very good.\n",
    "4. Make sure you are doing pre-processing in the right manner. For example, make sure that mean over entire training data is zero. For testing data, subtract the mean vector of the training data from each instance of testing data. Don't subtract the mean of testing data from itself. Since, you wouldn't know the mean of testing data at runtime.\n",
    "4. Check if your loss at the very first epoch makes sense. For example, in a 10-class classification problem, starting loss should be -ln(0.1) = 2.302 (given here).\n",
    "5. Again, from here, overfit a tiny subset of data and make sure you can achieve zero cost. Full details in the link.\n",
    "6. If nothing works, just train and test on the same data and see if you can get 90% + accuracy. Otherwise, examine your network more closely by looking at individual layer outputs (given in Keras FAQ) etc.\n",
    "\n",
    "\n",
    "<a href =\"https://github.com/keras-team/keras/issues/1761\">maybe solution</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
